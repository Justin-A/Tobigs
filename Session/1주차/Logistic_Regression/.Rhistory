setwd("C:/Users/Owner/Dropbox/tobigs10/1주차 수업")
##1
library(mlbench)
data("BreastCancer")
str(BreastCancer)
bc = na.omit(BreastCancer[-1])
bc = apply(bc, 2, as.numeric)
bc = data.frame(bc)
library(caret)
#Logistic Reg
accuracy.log = c()
for(i in 1:50){
idx = createDataPartition(bc$Class, p =0.7, list = F)
train = bc[idx,]
test = bc[-idx,]
model.log = glm(Class ~ ., data = train, family = binomial)
pred = as.numeric(predict(model.log, test, type = "response") > 0.5)
cfm = confusionMatrix(as.factor(test$Class), as.factor(pred))
accuracy.log = c(accuracy.log,cfm$overall[1])
}
setwd("C:/Users/Owner/Dropbox/tobigs10/1주차 수업")
##1
library(mlbench)
data("BreastCancer")
str(BreastCancer)
bc = na.omit(BreastCancer[-1])
bc$Class = as.numeric(bc$Class)-1
bc = apply(bc, 2, as.numeric)
bc = data.frame(bc)
library(caret)
#Logistic Reg
accuracy.log = c()
for(i in 1:50){
idx = createDataPartition(bc$Class, p =0.7, list = F)
train = bc[idx,]
test = bc[-idx,]
model.log = glm(Class ~ ., data = train, family = binomial)
pred = as.numeric(predict(model.log, test, type = "response") > 0.5)
cfm = confusionMatrix(as.factor(test$Class), as.factor(pred))
accuracy.log = c(accuracy.log,cfm$overall[1])
}
mean(accuracy.log)
for(i in 1:50){
idx = createDataPartition(bc$Class, p =0.7, list = F)
train = bc[idx,]
test = bc[-idx,]
model.log = glm(Class ~ ., data = train, family = binomial)
pred = as.numeric(predict(model.log, test, type = "response") > 0.5)
cfm = confusionMatrix(as.factor(test$Class), as.factor(pred))
accuracy.log = c(accuracy.log,cfm$overall[1])
}
for(i in 1:50){
idx = createDataPartition(bc$Class, p =0.7, list = F)
train = bc[idx,]
test = bc[-idx,]
model.log = glm(Class ~ ., data = train, family = binomial)
pred = as.numeric(predict(model.log, test, type = "response") > 0.5)
cfm = confusionMatrix(as.factor(test$Class), as.factor(pred))
accuracy.log = c(accuracy.log,cfm$overall[1])
}
for(i in 1:50){
idx = createDataPartition(bc$Class, p =0.7, list = F)
train = bc[idx,]
test = bc[-idx,]
model.log = glm(Class ~ ., data = train, family = binomial)
pred = as.numeric(predict(model.log, test, type = "response") > 0.5)
cfm = confusionMatrix(as.factor(test$Class), as.factor(pred))
accuracy.log = c(accuracy.log,cfm$overall[1])
}
for(i in 1:50){
idx = createDataPartition(bc$Class, p =0.7, list = F)
train = bc[idx,]
test = bc[-idx,]
model.log = glm(Class ~ ., data = train, family = binomial)
pred = as.numeric(predict(model.log, test, type = "response") > 0.5)
cfm = confusionMatrix(as.factor(test$Class), as.factor(pred))
accuracy.log = c(accuracy.log,cfm$overall[1])
}
##2
load("psub.RData")
data = psub
unique(data$SCHL)
str(data)
psub$AGEP
#학사학위 미만은 0으로
data[data$SCHL == "some college credit, no degree",]$bachdeg = 0
data[data$SCHL == "Regular high school diploma",]$bachdeg = 0
data[data$SCHL == "no high school diploma",]$bachdeg = 0
data[data$SCHL == "GED or alternative credential",]$bachdeg = 0
#수치형 데이터 전환
data$AGEP = as.numeric(data$AGEP)
data$PINCP = as.numeric(data$PINCP)
str(data)
psub$AGEP
#사용할 변수만 선택
data = subset(data, select = c(bachdeg, AGEP, SEX, COW, PINCP, SCHL))
library(caret)
#train/test 분할
idx = createDataPartition(data$SCHL, p = 0.7, list = F)
train = data[idx,]
test = data[-idx,]
#모델 적합
model = glm(bachdeg ~ AGEP + SEX + COW + PINCP , data = train, family = 'binomial')
#예측
test$pred = as.numeric(predict(model, test[1:5], type = 'response') > 0.5)
#모델 적합
model = glm(bachdeg ~ AGEP + SEX + COW + PINCP , data = train, family = 'binomial')
#bachdeg
data$bachdeg = 1
#학사학위 미만은 0으로
data[data$SCHL == "some college credit, no degree",]$bachdeg = 0
data[data$SCHL == "Regular high school diploma",]$bachdeg = 0
data[data$SCHL == "no high school diploma",]$bachdeg = 0
data[data$SCHL == "GED or alternative credential",]$bachdeg = 0
#수치형 데이터 전환
data$AGEP = as.numeric(data$AGEP)
data$PINCP = as.numeric(data$PINCP)
#사용할 변수만 선택
data = subset(data, select = c(bachdeg, AGEP, SEX, COW, PINCP, SCHL))
library(caret)
#train/test 분할
idx = createDataPartition(data$SCHL, p = 0.7, list = F)
train = data[idx,]
test = data[-idx,]
#모델 적합
model = glm(bachdeg ~ AGEP + SEX + COW + PINCP , data = train, family = 'binomial')
#예측
test$pred = as.numeric(predict(model, test[1:5], type = 'response') > 0.5)
#실제 값과 예측값 비교
confusionMatrix(as.factor(test$bachdeg), as.factor(test$pred))
setwd("C:/Users/Owner/Dropbox/tobigs10/1주차 수업")
#1
library(boot)
data(nodal)
str(nodal)
?nodal
table(nodal$m)
rd = nodal[,-1]
model = glm(r~., data = rd, family = binomial)
summary(model)
predict(model) #output은 logit
sigmoid = function(x) {
return(exp(x)/(1+exp(x)))
}
sigmoid(predict(model)) #sigmoid를 걸어야 원하는 확률값이 나온다!
predict(model, type = "response") #output은 sigmoid를 거친 값!
sigmoid(predict(model)) #sigmoid를 걸어야 원하는 확률값이 나온다!
predict(model, type = "response") #output은 sigmoid를 거친 값!
#2
#은행 예금에 가입할 것인지 예측을 위한 데이
bank = read.csv("bank-additional.csv", sep = ";")
str(bank)
#Feature Selection by Hand
select1 = colnames(bank)[c(1,2,3,6,7,8:10,12,15,17:19,21)]
select11 = colnames(bank)[c(1,2,3,6,7,8:10,12,15,17:19)]
formula1 = formula(paste("y~",paste(select11, collapse=" + ")))
select1
bank = bank[select1]
bank$y = as.factor(ifelse(bank$y == "no",0,1))
str(bank)
#train/test partition
library(caret)
idx = createDataPartition(bank$y, p = 0.7, list = F)
idx
banktrain = bank[idx,]
banktest = bank[-idx,]
formula1
##Model1
model.glm1 = glm(formula1, banktrain, family = binomial)
pred.glm1 = as.numeric(predict(model.glm1, banktest, type = "response") > 0.5)
confusionMatrix(as.factor(pred.glm1),as.factor(banktest$y))
##Model2
model.glm2 = glm(formula1, banktrain, family = binomial)
pred.glm2 = as.numeric(predict(model.glm2, banktest, type = "response") > 0.3)
confusionMatrix(as.factor(pred.glm2),as.factor(banktest$y))
#Upsample
table(banktrain$y)
banktrain_up = upSample(subset(banktrain, select=-y), banktrain$y)
table(banktrain_up$Class)
formula2 = formula(paste("Class~",paste(select11, collapse=" + ")))
##Model3
model.glm3 = glm(formula2, banktrain_up, family = binomial)
pred.glm3 = as.numeric(predict(model.glm3, banktest, type = "response") > 0.5)
confusionMatrix(as.factor(pred.glm3),banktest$y)
table(pred.glm3)
#ROC
library(ROCR)
pred_glm <- prediction(as.numeric(pred.glm3),as.numeric(banktest$y))
perf_glm <- performance(pred_glm, measure = "tpr", x.measure = "fpr")
plot(perf_glm, main = "ROC curve for GLM", col = "blue", lwd = 2)
#AUC
auc_glm = performance(pred_glm, measure = "auc")
auc_glm@y.values[[1]]
##1
library(mlbench)
data("BreastCancer")
str(BreastCancer)
bc = na.omit(BreastCancer[-1])
bc$Class = as.numeric(bc$Class)-1
bc = apply(bc, 2, as.numeric)
library(caret)
#Logistic Reg
accuracy.log = c()
for(i in 1:50){
idx = createDataPartition(bc$Class, p =0.7, list = F)
train = bc[idx,]
test = bc[-idx,]
model.log = glm(Class ~ ., data = train, family = binomial)
pred = as.numeric(predict(model.log, test, type = "response") > 0.5)
cfm = confusionMatrix(as.factor(test$Class), as.factor(pred))
accuracy.log = c(accuracy.log,cfm$overall[1])
}
bc = data.frame(bc)
mean(accuracy.log)
##1
library(mlbench)
data("BreastCancer")
str(BreastCancer)
bc = na.omit(BreastCancer[-1])
bc$Class = as.numeric(bc$Class)-1
bc = apply(bc, 2, as.numeric)
bc = data.frame(bc)
library(caret)
#Logistic Reg
accuracy.log = c()
for(i in 1:50){
idx = createDataPartition(bc$Class, p =0.7, list = F)
train = bc[idx,]
test = bc[-idx,]
model.log = glm(Class ~ ., data = train, family = binomial)
pred = as.numeric(predict(model.log, test, type = "response") > 0.5)
cfm = confusionMatrix(as.factor(test$Class), as.factor(pred))
accuracy.log = c(accuracy.log,cfm$overall[1])
}
mean(accuracy.log)
##2
load("psub.RData")
data = psub
unique(data$SCHL)
str(data)
#bachdeg
data$bachdeg = 1
#학사학위 미만은 0으로
data[data$SCHL == "some college credit, no degree",]$bachdeg = 0
data[data$SCHL == "Regular high school diploma",]$bachdeg = 0
data[data$SCHL == "no high school diploma",]$bachdeg = 0
#수치형 데이터 전환
data$AGEP = as.numeric(data$AGEP)
data[data$SCHL == "GED or alternative credential",]$bachdeg = 0
data$PINCP = as.numeric(data$PINCP)
#사용할 변수만 선택
data = subset(data, select = c(bachdeg, AGEP, SEX, COW, PINCP, SCHL))
library(caret)
#train/test 분할
idx = createDataPartition(data$SCHL, p = 0.7, list = F)
train = data[idx,]
test = data[-idx,]
#모델 적합
model = glm(bachdeg ~ AGEP + SEX + COW + PINCP , data = train, family = 'binomial')
#실제 값과 예측값 비교
confusionMatrix(as.factor(test$bachdeg), as.factor(test$pred))
#예측
test$pred = as.numeric(predict(model, test[1:5], type = 'response') > 0.5)
#모델 적합
model = glm(bachdeg ~ AGEP + SEX + COW + PINCP , data = train, family = 'binomial')
#예측
test$pred = as.numeric(predict(model, test[1:5], type = 'response') > 0.5)
#실제 값과 예측값 비교
confusionMatrix(as.factor(test$bachdeg), as.factor(test$pred))
setwd("C:/Users/Owner/Dropbox/tobigs10/1주차 수업")
#1
library(boot)
source('C:/Users/Owner/Dropbox/tobigs10/1주차 수업/Code.R', encoding = 'UTF-8', echo=TRUE)
setwd("C:/Users/Owner/Dropbox/tobigs10/1주차 수업")
#1
library(boot)
data(nodal)
str(nodal)
?nodal
table(nodal$m)
rd = nodal[,-1]
table(rd$r)
model = glm(r~., data = rd, family = binomial)
summary(model)
predict(model) #output은 logit
sigmoid = function(x) {
return(exp(x)/(1+exp(x)))
}
sigmoid(predict(model)) #sigmoid를 걸어야 원하는 확률값이 나온다!
predict(model, type = "response") #output은 sigmoid를 거친 값!
#2
#은행 예금에 가입할 것인지 예측을 위한 데이
bank = read.csv("bank-additional.csv", sep = ";")
str(bank)
#Feature Selection by Hand
select1 = colnames(bank)[c(1,2,3,6,7,8:10,12,15,17:19,21)]
select11 = colnames(bank)[c(1,2,3,6,7,8:10,12,15,17:19)]
formula1 = formula(paste("y~",paste(select11, collapse=" + ")))
bank = bank[select1]
bank$y = as.factor(ifelse(bank$y == "no",0,1))
bank$y
str(bank)
#train/test partition
library(caret)
idx = createDataPartition(bank$y, p = 0.7, list = F)
idx
banktrain = bank[idx,]
banktest = bank[-idx,]
formula1
##Model1
model.glm1 = glm(formula1, banktrain, family = binomial)
pred.glm1 = as.numeric(predict(model.glm1, banktest, type = "response") > 0.5)
confusionMatrix(as.factor(pred.glm1),as.factor(banktest$y))
##Model2
model.glm2 = glm(formula1, banktrain, family = binomial)
pred.glm2 = as.numeric(predict(model.glm2, banktest, type = "response") > 0.3)
confusionMatrix(as.factor(pred.glm2),as.factor(banktest$y))
#Upsample
table(banktrain$y)
banktrain_up = upSample(subset(banktrain, select=-y), banktrain$y)
table(banktrain_up$Class)
formula2 = formula(paste("Class~",paste(select11, collapse=" + ")))
##Model3
model.glm3 = glm(formula2, banktrain_up, family = binomial)
pred.glm3 = as.numeric(predict(model.glm3, banktest, type = "response") > 0.5)
confusionMatrix(as.factor(pred.glm3),banktest$y)
table(pred.glm3)
#ROC
library(ROCR)
pred_glm <- prediction(as.numeric(pred.glm3),as.numeric(banktest$y))
perf_glm <- performance(pred_glm, measure = "tpr", x.measure = "fpr")
plot(perf_glm, main = "ROC curve for GLM", col = "blue", lwd = 2)
#AUC
auc_glm = performance(pred_glm, measure = "auc")
auc_glm@y.values[[1]]
ㅊ
C
Sys.setlocale(“LC_CTYPE”, “ko_KR.UTF-8”)
Sys.setlocale('LC_CTYPE', 'ko_KR.UTF-8')
setwd("~/Desktop/Justin/Tobigs/10/1주차/Logistic_Regression/")
#1
library(boot)
data(nodal)
str(nodal)
?nodal
table(nodal$m)
model = glm(r~., data = rd, family = binomial)
rd = nodal[,-1]
table(nodal$m)
rd = nodal[,-1]
table(rd$r)
model = glm(r~., data = rd, family = binomial)
summary(model)
sigmoid(predict(model)) #sigmoid를 걸어야 원하는 확률값이 나온다!
sigmoid = function(x) {
return(exp(x)/(1+exp(x)))
}
sigmoid(predict(model)) #sigmoid를 걸어야 원하는 확률값이 나온다!
predict(model, type = "response") #output은 sigmoid를 거친 값!
#2
#은행 예금에 가입할 것인지 예측을 위한 데이
bank = read.csv("bank-additional.csv", sep = ";")
str(bank)
#Feature Selection by Hand
select1 = colnames(bank)[c(1,2,3,6,7,8:10,12,15,17:19,21)]
#Feature Selection by Hand
select1 = colnames(bank)[c(1,2,3,6,7,8:10,12,15,17:19,21)]
select11 = colnames(bank)[c(1,2,3,6,7,8:10,12,15,17:19)]
formula1 = formula(paste("y~",paste(select11, collapse=" + ")))
bank = bank[select1]
bank$y = as.factor(ifelse(bank$y == "no",0,1))
str(bank)
bank$y = as.factor(ifelse(bank$y == "no",0,1)) # YES, NO -> 0, 1
str(bank)
#train/test partition
library(caret)
idx = createDataPartition(bank$y, p = 0.7, list = F)
banktrain = bank[idx,]
banktest = bank[-idx,]
##Model1
model.glm1 = glm(formula1, banktrain, family = binomial)
pred.glm1 = as.numeric(predict(model.glm1, banktest, type = "response") > 0.5)
confusionMatrix(as.factor(pred.glm1),as.factor(banktest$y))
table(pred.glm1)
##Model2
model.glm2 = glm(formula1, banktrain, family = binomial)
pred.glm2 = as.numeric(predict(model.glm2, banktest, type = "response") > 0.3)
confusionMatrix(as.factor(pred.glm2),as.factor(banktest$y))
table(pred.glm2)
#Upsample
table(banktrain$y)
banktrain_up = upSample(subset(banktrain, select=-y), banktrain$y)
table(banktrain_up$Class)
formula2 = formula(paste("Class~",paste(select11, collapse=" + ")))
##Model3
model.glm3 = glm(formula2, banktrain_up, family = binomial)
pred.glm3 = as.numeric(predict(model.glm3, banktest, type = "response") > 0.5)
confusionMatrix(as.factor(pred.glm3),banktest$y)
table(pred.glm3)
#train/test partition
library(caret)
idx = createDataPartition(bank$y, p = 0.7, list = F)
banktrain = bank[idx,]
banktest = bank[-idx,]
##Model1
model.glm1 = glm(formula1, banktrain, family = binomial)
pred.glm1 = as.numeric(predict(model.glm1, banktest, type = "response") > 0.5)
##Model1
model.glm1 = glm(formula1, banktrain, family = binomial)
pred.glm1 = as.numeric(predict(model.glm1, banktest, type = "response") > 0.5)
confusionMatrix(as.factor(pred.glm1),as.factor(banktest$y))
