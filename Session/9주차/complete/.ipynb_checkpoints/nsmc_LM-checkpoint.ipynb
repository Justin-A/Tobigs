{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NSMC language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import torchtext\n",
    "from konlpy.tag import Mecab\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset, Dataset\n",
    "import os\n",
    "\n",
    "from cnn_model import CNNClassifier\n",
    "from rnn_model import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리를 위해 torchtext 라이브러리를 사용한다. 텍스트를 전처리해서 단어셋 구축과 숫자화(numericalize) 역할까지 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/Users/kwang/Project/ToBigs/2018-2/Lecture_NLP/complete' #os.environ['DATA_PATH']\n",
    "tagger = Mecab()\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "#DEVICE = 'cuda' if USE_CUDA else 'cpu'\n",
    "DEVICE = 0 if USE_CUDA else -1\n",
    "\n",
    "def pad_under_five(toknized):\n",
    "    \"\"\"\n",
    "    모델에서 5-gram 단위 필터를 사용하기 때문에\n",
    "    5-gram이 안되는 문장에 <pad>로 채워준다\n",
    "    \"\"\"\n",
    "    if len(toknized) < 5:\n",
    "        toknized.extend([\"<pad>\"]*(5-len(toknized)))\n",
    "    return toknized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한글 형태소 분석기로 mecab(konlpy)를 이용한다.\n",
    "\n",
    "최소 문장 길이만큼 패딩해주는 전처리 함수를 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(tokenize=tagger.morphs, lower=True, init_token=\"<s>\",\n",
    "             eos_token=\"</s>\", include_lengths=True,\n",
    "             batch_first=True, preprocessing=pad_under_five) \n",
    "\n",
    "LABEL = Field(sequential=False,use_vocab=True,unk_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트와 정답을 Field로 정의한다. 텍스트는 mecab 형태소 단위로 분리되며 패딩 전처리 함수 또한 여기서 정의한다.  \n",
    "\n",
    "라벨은 use_vocab 파라미터를 사용해서 일반 텍스트도 정답으로 사용될 수 있다.  \n",
    "\n",
    "Field 내에는 vocab이라는 객체가 정의되어 있다. 다만, Field에 실제 데이터를 할당한 이후부터 사용 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 레벨 문장의 길이가 1 이상인 경우만 허용\n",
    "train_data, test_data = TabularDataset.splits(path=DATA_PATH+'/nsmc/',\n",
    "                                              train='ratings_train.txt',\n",
    "                                              test='ratings_test.txt',\n",
    "                                              format='tsv', \n",
    "                                              skip_header=True, \n",
    "                                              fields=[('id',None),('text',TEXT),('label',LABEL)], \n",
    "                                              filter_pred = lambda x: True if len(x.text) > 1 else False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TabularDataset은 실제 텍스트 파일(json, txt, csv 등)을 읽어 field에 데이터를 할당하는 역할을 한다.\n",
    "\n",
    "이제 TEXT와 LABEL에 실제 데이터가 할당 되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000 50000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data,min_freq=2)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build_vocab이라는 메소드를 이용해서 vocab을 구축할 수 있다. 최소 빈도를 지정하여 코퍼스 구축이 가능하다.  \n",
    "\n",
    "이제 단어셋을 아래처럼 찍어볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29976"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '<s>', '</s>', '.', '이', '는', '영화', '다', '고', '하', '도', '의', '가', '은', '에', '을', '보', '한', '..', '게', ',', '들', '!', '지', '를', '있', '없', '?', '좋', '나', '었', '만', '는데', '너무', '봤', '적', '안', '정말', '로', '음', '으로', '것', '아', '네요', '재밌', '점', '어', '같', '지만', '진짜', '했', '에서', '기', '네', '않', '거', '았', '수', '되', '면', '과', '말', '연기', '인', '주', '잘', '최고', '~', '내', '평점', '이런', '던', '어요', '와', '생각', 'ㅎ', '할', '왜', '1', '겠', '스토리', '습니다', '해', '...', '드라마', '아니', '싶', '그', '사람', '듯', '함', '더', '감동', '때', '배우', '본', '까지', '좀', '뭐'] 29976\n"
     ]
    }
   ],
   "source": [
    "print (TEXT.vocab.itos[:100], len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset 객체에서 example 멤버를 이용하면 전처리된 실제 데이터에 순차적으로 접근할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아', '더빙', '.', '.', '진짜', '짜증', '나', '네요', '목소리'] 0\n",
      "['걍인피니트가짱이다', '.', '진짜', '짱', '이', '다', '♥'] 1\n",
      "['신카이', '마코토', '의', '작화', '와', ',', '미유', '와', '하나', '카', '나', '가', '연기', '를', '잘', '해', '줘서', '더', '대박', '이', '였', '다', '.'] 1\n"
     ]
    }
   ],
   "source": [
    "print (train_data.examples[0].text, train_data.examples[0].label)\n",
    "print (train_data.examples[10].text, train_data.examples[10].label)\n",
    "print (train_data.examples[100].text, train_data.examples[100].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 160207\n",
      "0 4\n",
      "<unk> <s>\n",
      "54802 29976 29976\n"
     ]
    }
   ],
   "source": [
    "print (TEXT.vocab.freqs['<unk>'], TEXT.vocab.freqs['.'])\n",
    "print (TEXT.vocab.stoi['<unk>'], TEXT.vocab.stoi['.'])\n",
    "print (TEXT.vocab.itos[0], TEXT.vocab.itos[2])\n",
    "print (len(TEXT.vocab.freqs), len(TEXT.vocab.itos), len(TEXT.vocab.stoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab 멤버로는 freq, stoi, itos가 있으며 각각 빈도, string to int, int to string을 나타낸다.  \n",
    "\n",
    "build_vocab에서 freq 제한을 두었기 때문에 실제 구축된 vocab의 길이가 freq 리스트의 길이보다 짧은 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset으로 읽은 데이터를 이용해서 연산 가능한 형태로 만든다. iterator화 해서 학습에 유용한 loader로 변환한다.  \n",
    "\n",
    "loader는 batch 단위로 접근이 가능하다. 문장은 numericalized된 long type 데이터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = BucketIterator.splits((train_data, test_data),  \n",
    "                                                  sort_key=lambda x:len(x.text),\n",
    "                                                  sort_within_batch=True,\n",
    "                                                  repeat=False,shuffle=True,\n",
    "                                                  batch_size=32,device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[     2,   1704,   6193,     55,     49,     21,   7390,     32,\n",
      "              5,    284,    135,     58,     26,      6,   6943,     18,\n",
      "            143,      5,     67,      8,      3],\n",
      "        [     2,   2635,  13617,  22108,     12,   2946,      6,   1368,\n",
      "             91,      5,     31,    891,      4,     50,    986,   2172,\n",
      "             20,     66,   1359,      8,      3],\n",
      "        [     2,   4003,      4,     84,    321,   1177,     10,     24,\n",
      "             11,     55,      9,      4,     84,   1281,     11,     99,\n",
      "           8068,     36,      4,     19,      3],\n",
      "        [     2,    538,     52,   2092,    382,     96,     89,      5,\n",
      "             26,    326,    833,    328,     88,    170,      5,    549,\n",
      "           1245,    288,      4,     19,      3],\n",
      "        [     2,    106,     32,     26,   1386,      4,   1711,    556,\n",
      "          15039,     15,    392,     72,   1069,   1940,     11,    210,\n",
      "             43,     26,      9,      4,      3],\n",
      "        [     2,    290,  13226,   9955,    158,    304,    307,    129,\n",
      "            373,   1003,      7,    139,    417,     11,    106,     11,\n",
      "            240,     11,     27,     44,      3],\n",
      "        [     2,   5818,  21216,    671,     60,      0,    539,   3726,\n",
      "             83,     65,    234,      4,    228,   4826,   1698,      4,\n",
      "           5485,    105,     11,    407,      3],\n",
      "        [     2,   7700,  15912,     61,   8398,   6844,  11613,      4,\n",
      "           4252,     11,   1410,     49,     88,     22,     12,     63,\n",
      "              6,    976,   1410,      8,      3],\n",
      "        [     2,   1366,     12,   3373,      5,   7368,      0,    398,\n",
      "            177,   5223,     15,    309,   3638,     13,    725,     10,\n",
      "             80,   1503,      4,     19,      3],\n",
      "        [     2,    304,   2055,      6,   2563,      4,     19,    276,\n",
      "           8979,     22,     12,     71,    809,    883,      4,     19,\n",
      "            106,   1198,  10882,     23,      3],\n",
      "        [     2,   2025,     18,   8899,     25,   1886,    189,      7,\n",
      "             76,     76,    191,   2934,      6,   2578,      5,     44,\n",
      "              4,     34,     45,     73,      3],\n",
      "        [     2,     43,     30,    134,     17,      9,     87,     47,\n",
      "            963,     80,    184,     23,   5686,   1238,    382,     69,\n",
      "           1405,    300,     23,   1972,      3],\n",
      "        [     2,     17,     53,     15,    856,     18,      7,      4,\n",
      "             19,    142,     99,   2307,    114,    157,      7,     64,\n",
      "             56,    268,    121,     28,      3],\n",
      "        [     2,     74,     71,    189,      5,    390,     46,     11,\n",
      "             37,     59,     54,    167,    191,    706,      6,    393,\n",
      "             16,     20,    118,    355,      3],\n",
      "        [     2,    600,   7138,     61,   4483,     16,   2320,   1334,\n",
      "           1913,   1844,    297,      9,    251,     18,    215,     39,\n",
      "             85,    323,     51,    405,      3],\n",
      "        [     2,     79,    165,    108,    125,    165,      5,      4,\n",
      "              4,    125,    165,    108,    145,    165,      5,      4,\n",
      "              4,    106,     27,     54,      3],\n",
      "        [     2,    612,    721,    106,     26,     47,     24,   2520,\n",
      "             10,    128,    125,   1298,    145,    166,     96,      7,\n",
      "           8438,     71,   5921,     23,      3],\n",
      "        [     2,    717,    237,     11,    303,     58,    505,     27,\n",
      "              6,    435,     23,     23,     71,     20,    746,    189,\n",
      "              5,     24,     23,     23,      3],\n",
      "        [     2,    125,    112,      5,   1306,    181,      6,     20,\n",
      "           1042,      5,     49,     21,    253,      5,    264,     26,\n",
      "              6,      7,     23,   1851,      3],\n",
      "        [     2,   4481,    114,     79,   1658,     97,      7,    113,\n",
      "             15,     67,      8,     23,    167,   5295,    178,      5,\n",
      "           1536,    297,      8,     23,      3],\n",
      "        [     2,   2136,    318,      5,    140,      6,      4,      4,\n",
      "             34,     29,     44,   1554,     35,     33,    196,     17,\n",
      "              9,     87,     14,      4,      3],\n",
      "        [     2,    704,   1834,    476,     30,   3244,    525,     15,\n",
      "             74,    522,     67,     12,    320,      7,   1337,     50,\n",
      "            116,   1878,     68,    109,      3],\n",
      "        [     2,   1154,    729,    554,    660,    339,   3030,     14,\n",
      "            163,     11,   1348,     77,     58,     27,      8,      4,\n",
      "            353,    123,   1900,    383,      3],\n",
      "        [     2,    723,     16,     94,      6,     50,     45,     31,\n",
      "             33,    163,    130,     17,   1376,    176,    106,     13,\n",
      "             27,     54,      4,     19,      3],\n",
      "        [     2,    182,    645,    145,     12,    115,    323,     13,\n",
      "            255,   1706,      8,      4,   3062,     18,   1208,     12,\n",
      "           9818,     13,      6,     85,      3],\n",
      "        [     2,   1680,     16,    670,   3308,   1736,      0,   1140,\n",
      "              5,   1278,      6,   1377,    290,      6,    163,     11,\n",
      "           1025,    849,    478,      4,      3],\n",
      "        [     2,  17674,    299,   2589,    868,     39,      4,     71,\n",
      "            120,    174,    389,     14,   6991,     55,    250,     34,\n",
      "             29,     57,     73,      4,      3],\n",
      "        [     2,   2159,     39,  28058,     12,   6799,    173,     16,\n",
      "           8731,     10,    710,     88,   9445,    201,     13,  10821,\n",
      "           3361,      5,      8,      4,      3],\n",
      "        [     2,    595,    783,     17,     60,      8,    185,     10,\n",
      "           1579,    584,    107,   1053,      4,      4,    313,     79,\n",
      "            108,    122,     40,      4,      3],\n",
      "        [     2,    500,     52,   1893,     83,     65,    751,    115,\n",
      "            150,     35,      8,      4,   3530,    314,     38,      4,\n",
      "             19,     43,      4,     19,      3],\n",
      "        [     2,   1570,     15,     35,     33,   2363,     94,     97,\n",
      "             34,   1706,     73,      4,      4,    275,    456,     15,\n",
      "           2218,     18,      7,     23,      3],\n",
      "        [     2,    825,     49,   1201,      7,    167,      4,    167,\n",
      "            334,    342,     61,     34,    244,    104,  14223,   1084,\n",
      "            225,   3126,   2756,      0,      3]]), tensor([ 21,  21,  21,  21,  21,  21,  21,  21,  21,  21,  21,  21,\n",
      "         21,  21,  21,  21,  21,  21,  21,  21,  21,  21,  21,  21,\n",
      "         21,  21,  21,  21,  21,  21,  21,  21]))\n"
     ]
    }
   ],
   "source": [
    "for bt in train_loader:\n",
    "    print(bt.text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from attention import Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size,embed_size,hidden_size,output_size,\n",
    "                 num_layers=1,bidirec=False,vocab=None):\n",
    "        super(RNN,self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        if bidirec:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "            \n",
    "        self.embed = nn.Embedding(input_size,embed_size)\n",
    "#         self.embed.weight.data.copy_(vocab.vectors)\n",
    "        self.lstm = nn.LSTM(embed_size,hidden_size,num_layers,batch_first=True,bidirectional=bidirec)\n",
    "        self.linear = nn.Linear(hidden_size*self.num_directions,output_size)\n",
    "#         self.attention = Attention(self.hidden_size*self.num_directions, method='general')\n",
    "        \n",
    "    def init_hidden(self,batch_size):\n",
    "        # (num_layers * num_directions, batch_size, hidden_size)\n",
    "        \n",
    "        hidden = Variable(torch.zeros(self.num_layers*self.num_directions,batch_size,self.hidden_size)).cuda()\n",
    "        cell = Variable(torch.zeros(self.num_layers*self.num_directions,batch_size,self.hidden_size)).cuda()\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self,inputs, encoder_length=None):\n",
    "        \"\"\"\n",
    "        inputs : B,T\n",
    "        \"\"\"\n",
    "        embed = self.embed(inputs) # word vector indexing\n",
    "        hidden, cell = self.init_hidden(inputs.size(0)) # initial hidden,cell\n",
    "        \n",
    "        output, h = self.lstm(embed,(hidden,cell)) # output : Batch,Time,Hidden (32, 9, 200)\n",
    "        hidden, cell = h # hidden : L, B, H\n",
    "        \n",
    "        hidden = hidden[-self.num_directions:] # (num_directions,B,H)\n",
    "        hidden = torch.cat([h for h in hidden],1) #.unsqueeze(0) # (1,B,2H) (1, 32, 200)\n",
    "        \n",
    "#         print (output.size(), hidden.size(), len(encoder_length))\n",
    "#         print (encoder_length, type(encoder_length[0]))\n",
    "        \n",
    "        # context : Batch, 1, Hidden*bi\n",
    "#         context, attn_weight = self.attention(hidden.transpose(0,1), output, encoder_length, return_weight=True)\n",
    "#         print (context.size())\n",
    "        \n",
    "        # Many-to-One\n",
    "        output = self.linear(hidden) # last hidden\n",
    "        output = output.squeeze(1)\n",
    "#         print (output.size())\n",
    "        \n",
    "#         return F.softmax(output)\n",
    "        return F.log_softmax(output, dim=1)#, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 10\n",
    "BATCH_SIZE = 32\n",
    "EMBED = 200\n",
    "KERNEL_DIM = 100\n",
    "LR = 0.001\n",
    "output_size = 2\n",
    "\n",
    "# model = CNNClassifier(len(TEXT.vocab), EMBED, 1, KERNEL_DIM, KERNEL_SIZES)\n",
    "model = RNN(len(TEXT.vocab), EMBED, KERNEL_DIM, output_size, num_layers=2,\n",
    "            bidirec=True, vocab=TEXT.vocab)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5], gamma=0.1)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cuda.FloatTensor is not enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-7125624a95c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_P\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_P\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-a7ffb530f71d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, encoder_length)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \"\"\"\n\u001b[1;32m     30\u001b[0m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# word vector indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# initial hidden,cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# output : Batch,Time,Hidden (32, 9, 200)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-a7ffb530f71d>\u001b[0m in \u001b[0;36minit_hidden\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# (num_layers * num_directions, batch_size, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_directions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_directions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cuda.FloatTensor is not enabled."
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    scheduler.step()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, lengths = batch.text\n",
    "        targets = batch.label\n",
    "        # prevent length = 0\n",
    "        if 0 in lengths:\n",
    "            idxes = torch.arange(inputs.size(0))\n",
    "            if USE_CUDA:\n",
    "                idxes = idxes.cuda()\n",
    "            mask = Variable(idxes[lengths.ne(0)].long())\n",
    "\n",
    "            inputs = inputs.index_select(0, mask)\n",
    "            lengths = lengths.masked_select(lengths.ne(0))\n",
    "            targets = targets.index_select(0, mask)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        preds, loss_P = model(inputs, lengths)\n",
    "        \n",
    "        loss = loss_function(preds.view(-1), targets.float()) + loss_P\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    string = '[{}/{}] loss: {:.4f}'.format(step+1, STEP, np.mean(losses))\n",
    "    print(string)\n",
    "    losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cuda.FloatTensor is not enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-048ff7a93683>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_P\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_P\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-a7ffb530f71d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, encoder_length)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \"\"\"\n\u001b[1;32m     30\u001b[0m         \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# word vector indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# initial hidden,cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# output : Batch,Time,Hidden (32, 9, 200)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-a7ffb530f71d>\u001b[0m in \u001b[0;36minit_hidden\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# (num_layers * num_directions, batch_size, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_directions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_directions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.cuda.FloatTensor is not enabled."
     ]
    }
   ],
   "source": [
    "### train\n",
    "model.train()\n",
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    scheduler.step()\n",
    "    \n",
    "    for i,batch in enumerate(train_loader):\n",
    "        inputs, lengths = batch.text\n",
    "        targets = batch.label#.float()\n",
    "        # print (inputs.size())\n",
    "    #    if USE_CUDA:\n",
    "    #        inputs = inputs.cuda()\n",
    "    #        targets = targets.cuda()\n",
    "     \n",
    "    \n",
    "        if 0 in lengths:\n",
    "            idxes = torch.arange(inputs.size(0))\n",
    "            if USE_CUDA:\n",
    "                idxes = idxes.cuda()\n",
    "            mask = Variable(idxes[lengths.ne(0)].long())\n",
    "\n",
    "            inputs = inputs.index_select(0, mask)\n",
    "            lengths = lengths.masked_select(lengths.ne(0))\n",
    "            targets = targets.index_select(0, mask)\n",
    "    \n",
    "        model.zero_grad()\n",
    "        # model.batch_size = inputs.size()[0] # N, sent\n",
    "        # model.hidden = model.init_hidden()\n",
    "        # preds = model(inputs.t()) # sent, N\n",
    "#         preds, attn_weights = model(inputs, encoder_length=lengths.tolist())\n",
    "\n",
    "        \n",
    "        preds, loss_P = model(inputs, lengths)\n",
    "        \n",
    "        loss = loss_function(preds.view(-1), targets.float()) + loss_P\n",
    "        losses.append(loss.data[0])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    string = '[{}/{}] loss: {:.4f}'.format(step+1, STEP, np.mean(losses))\n",
    "    print(string)\n",
    "    losses = []\n",
    "\n",
    "\n",
    "        #preds = model(inputs, encoder_length=lengths.tolist())\n",
    "#        preds, loss_P = model(inputs, lengths)\n",
    "        \n",
    "        # print (preds, targets)\n",
    "        # print (preds.squeeze(1).size(), targets.size())\n",
    "#         loss = loss_function(preds.squeeze(1),targets)\n",
    "#        loss = loss_function(preds,targets)\n",
    "#        losses.append(loss.item()) #data[0])\n",
    "        # exit()\n",
    "#        loss.backward()\n",
    "#        optimizer.step()\n",
    "#         break\n",
    "#        if i % 1000 == 0:\n",
    "#            print(\"epoch : %d mean_loss : %.3f , lr : %.5f\" % (epoch,np.mean(losses), scheduler.get_lr()[0]))\n",
    "#            losses=[]\n",
    "#             torch.save(model.state_dict(), './model/rnn_news_text%03d.pth'%(i//1000))\n",
    "#     break\n",
    "torch.save(model.state_dict(), './model/nsmc_lm.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### evaluate\n",
    "model.eval()\n",
    "num_hit=0\n",
    "for i,batch in enumerate(test_loader):\n",
    "    inputs, lengths = batch.text\n",
    "    targets = batch.label#.float()\n",
    "    \n",
    "#     print ('\\n',inputs.size())\n",
    "#     print (inputs)\n",
    "#     break\n",
    "    if USE_CUDA:\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "#     inputs = inputs\n",
    "#     targets = ((targets-1)/4).round()\n",
    "#     output, attn_weights = model(inputs, lengths)\n",
    "    output = model(inputs, lengths)\n",
    "#     output = output.squeeze(1)\n",
    "    preds = output.max(1, keepdim=True)[1]\n",
    "#     print (preds.size())\n",
    "#     print (preds)\n",
    "#     print (targets)\n",
    "#     preds = preds*5\n",
    "#     preds = preds.round()\n",
    "    num_hit+=torch.eq(preds.squeeze(),targets.squeeze()).sum().item() #data[0]\n",
    "\n",
    "print('test accuracy: %.3f%%'%(num_hit/len(test_data)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test\n",
    "test_inputs = [\"개별적인 장면이 좋았다.\", \"존맛탱!\", \"헐 진짜 개별로다..\", \"진짜 너무 재밌는 영화다 오랜만에\",\"오..이건 진짜 봐야함\", \"진짜 쓰레기 같은 영화\",\"노잼\",\"존잼\",\"꾸울잼\",\"핵노잼\",'또 보고싶다', '꼬옥 봐야한다.. 진짜..', '나만 보기 아깝다', '돈이 아깝다', '나만 보기 억울하다', '나만 당할 수 없다', '너도 봐야한다', '혼자 본게 정말 후회된다. 이건 꼭 같이 봐야한다.', '재미없어요...', '꾸르르르르르르잼', '꾸르르르잼', '꾸르잼', '이 영화를 보고 암이 나았습니다.']\n",
    "\n",
    "for test_input in test_inputs:\n",
    "    tokenized = tagger.morphs(test_input)\n",
    "    length = len(tokenized)\n",
    "    tokenized = pad_under_five(tokenized)\n",
    "    input_, lengths = TEXT.numericalize(([tokenized], length), device=DEVICE)\n",
    "    if USE_CUDA: input_ = input_.cuda()\n",
    "    \n",
    "#     output, attn_weights = model(input_, [lengths.tolist()])\n",
    "    output = model(input_, [lengths.tolist()])\n",
    "#     print (tokenized)\n",
    "#     print (attn_weights.squeeze().data.cpu().numpy())\n",
    "#     print (output)\n",
    "    prediction = output.max(1, keepdim=True)[1]\n",
    "#     print (prediction)\n",
    "    if prediction[0][0] == 1:\n",
    "        print(test_input,\"\\033[1;01;36m\" + '긍정' + \"\\033[0m\")\n",
    "    else:\n",
    "        print(test_input,\"\\033[1;01;31m\" + '부정' + \"\\033[0m\")\n",
    "    print ()\n",
    "    \n",
    "print (LABEL.vocab.itos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), './model/rnn_self_text.pth')\n",
    "torch.save(TEXT.vocab, './model/news_field.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (LABEL.numericalize(['스포츠']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocaburay = torch.load('./model/vocab.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (vocaburay.itos[:10])\n",
    "TEST = Field(tokenize=tagger.morphs,lower=True,include_lengths=False,batch_first=True,preprocessing=pad_under_five)\n",
    "TEST.build_vocab()\n",
    "TEST.vocab = vocaburay\n",
    "TEST.numericalize(test_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
