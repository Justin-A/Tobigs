{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nsmc language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import torchtext\n",
    "from konlpy.tag import Mecab\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset, Dataset\n",
    "import os\n",
    "\n",
    "from cnn_model import CNNClassifier\n",
    "from rnn_model import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리를 위해 torchtext 라이브러리를 사용한다. 텍스트를 전처리해서 단어셋 구축과 숫자화(numericalize) 역할까지 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data' #os.environ['DATA_PATH']\n",
    "tagger = Mecab()\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = 'cuda' if USE_CUDA else 'cpu'\n",
    "\n",
    "def pad_under_five(toknized):\n",
    "    \"\"\"\n",
    "    모델에서 5-gram 단위 필터를 사용하기 때문에\n",
    "    5-gram이 안되는 문장에 <pad>로 채워준다\n",
    "    \"\"\"\n",
    "    if len(toknized) < 5:\n",
    "        toknized.extend([\"<pad>\"]*(5-len(toknized)))\n",
    "    return toknized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한글 형태소 분석기로 mecab(konlpy)를 이용한다.\n",
    "\n",
    "최소 문장 길이만큼 패딩해주는 전처리 함수를 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(tokenize=tagger.morphs,lower=True,init_token=\"<s>\",eos_token=\"</s>\",include_lengths=True,batch_first=True,preprocessing=pad_under_five)\n",
    "LABEL = Field(sequential=False,use_vocab=True,unk_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트와 정답을 Field로 정의한다. 텍스트는 mecab 형태소 단위로 분리되며 패딩 전처리 함수 또한 여기서 정의한다.\n",
    "\n",
    "라벨은 use_vocab 파라미터를 사용해서 일반 텍스트도 정답으로 사용될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Field 내에는 vocab이라는 객체가 정의되어 있다. 다만, Field에 실제 데이터를 할당한 이후부터 사용 가능하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = TabularDataset.splits(path=DATA_PATH+'/nsmc/',\n",
    " train='ratings_train.txt',\n",
    " test='ratings_test.txt',\n",
    " format='tsv', \n",
    " skip_header=True, \n",
    " fields=[('id',None),('text',TEXT),('label',LABEL)], \n",
    " filter_pred = lambda x: True if len(x.text) > 1 else False) \n",
    "# 토큰 레벨 문장의 길이가 1 이상인 경우만 허용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TabularDataset은 실제 텍스트 파일(json, txt, csv 등)을 읽어 field에 데이터를 할당하는 역할을 한다.\n",
    "\n",
    "이제 TEXT와 LABEL에 실제 데이터가 할당 되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def init_emb(vocab, init=\"randn\", num_special_toks=2):\n",
    "#     emb_vectors = vocab.vectors\n",
    "#     sweep_range = len(vocab)\n",
    "#     running_norm = 0.\n",
    "#     num_non_zero = 0\n",
    "#     total_words = 0\n",
    "#     for i in range(num_special_toks, sweep_range):\n",
    "#         if len(emb_vectors[i, :].nonzero()) == 0:\n",
    "#             # std = 0.05 is based on the norm of average GloVE 100-dim word vectors\n",
    "#             if init == \"randn\":\n",
    "#                 torch.nn.init.normal(emb_vectors[i], mean=0, std=0.05)\n",
    "#         else:\n",
    "#             num_non_zero += 1\n",
    "#             running_norm += torch.norm(emb_vectors[i])\n",
    "#         total_words += 1\n",
    "#     logger.info(\"average GloVE norm is {}, number of known words are {}, total number of words are {}\".format(\n",
    "#         running_norm / num_non_zero, num_non_zero, total_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16094 [00:00<?, ?it/s]Skipping token 16093 with 1-dimensional vector ['200']; likely a header\n",
      "100%|██████████| 16094/16094 [00:00<00:00, 19559.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# from torchtext import vocab\n",
    "# vec = vocab.Vectors('./data/nsmc/nsmc_model.vec', './data/nsmc')\n",
    "# TEXT.build_vocab(train_data, vectors=vec, unk_init=lambda x: init_emb(x, 'uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data,min_freq=2)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build_vocab이라는 메소드를 이용해서 vocab을 구축할 수 있다. 최소 빈도를 지정하여 코퍼스 구축이 가능하다.\n",
    "\n",
    "이제 단어셋을 아래처럼 찍어볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '<s>', '</s>', '.', '이', '는', '영화', '다', '고', '하', '도', '의', '가', '은', '에', '을', '보', '한', '..', '게', ',', '들', '!', '지', '를', '있', '없', '?', '좋', '나', '었', '만', '는데', '너무', '봤', '적', '안', '정말', '로', '음', '으로', '것', '아', '네요', '재밌', '점', '어', '같', '지만', '진짜', '했', '에서', '기', '네', '않', '거', '았', '수', '되', '면', '과', '말', '연기', '인', '주', '잘', '최고', '~', '내', '평점', '이런', '던', '어요', '와', '생각', 'ㅎ', '할', '왜', '1', '겠', '스토리', '습니다', '해', '...', '드라마', '아니', '싶', '그', '사람', '듯', '함', '더', '감동', '때', '배우', '본', '까지', '좀', '뭐'] 29976\n"
     ]
    }
   ],
   "source": [
    "print (TEXT.vocab.itos[:100], len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset 객체에서 example 멤버를 이용하면 전처리된 실제 데이터에 순차적으로 접근할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아', '더빙', '.', '.', '진짜', '짜증', '나', '네요', '목소리'] 0\n",
      "['걍인피니트가짱이다', '.', '진짜', '짱', '이', '다', '♥'] 1\n"
     ]
    }
   ],
   "source": [
    "print (train_data.examples[0].text, train_data.examples[0].label)\n",
    "print (train_data.examples[10].text, train_data.examples[10].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 160207\n",
      "0 2\n",
      "<unk> .\n",
      "54802 29974 29974\n"
     ]
    }
   ],
   "source": [
    "print (TEXT.vocab.freqs['<unk>'], TEXT.vocab.freqs['.'])\n",
    "print (TEXT.vocab.stoi['<unk>'], TEXT.vocab.stoi['.'])\n",
    "print (TEXT.vocab.itos[0], TEXT.vocab.itos[2])\n",
    "print (len(TEXT.vocab.freqs), len(TEXT.vocab.itos), len(TEXT.vocab.stoi))\n",
    "# min(TEXT.vocab.freqs)\n",
    "# print ({k:d[k] for k in TEXT.vocab.freqs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab 멤버로는 freq, stoi, itos가 있으며 각각 빈도, string to int, int to string을 나타낸다.\n",
    "\n",
    "build_vocab에서 freq 제한을 두었기 때문에 실제 구축된 vocab의 길이가 freq 리스트의 길이보다 짧은 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterator\n",
    "\n",
    "dataset으로 읽은 데이터를 이용해서 연산 가능한 형태로 만든다. iterator화 해서 학습에 유용한 loader로 변환한다.\n",
    "\n",
    "loader는 batch 단위로 접근이 가능하다. 문장은 numericalized된 long type 데이터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[     2,    397,     36,     41,    176,      4,     19,      3],\n",
      "        [     2,    192,     18,    190,   1616,     51,     54,      3],\n",
      "        [     2,    880,    281,    119,      7,      4,     19,      3],\n",
      "        [     2,    526,     16,     58,     27,      6,    189,      3],\n",
      "        [     2,     67,     12,      7,    113,    132,      4,      3],\n",
      "        [     2,   1294,     50,    251,     91,    101,     91,      3],\n",
      "        [     2,  11448,    242,     23,   1207,   1220,      0,      3],\n",
      "        [     2,    253,    111,     20,     35,      8,     68,      3],\n",
      "        [     2,    744,      0,     47,  17647,   1337,  29967,      3],\n",
      "        [     2,    990,     14,     86,      9,   1685,    109,      3],\n",
      "        [     2,    195,     67,     12,   1194,      7,      8,      3],\n",
      "        [     2,    604,   1629,    604,   1629,    604,   1629,      3],\n",
      "        [     2,  20697,     32,   3102,     32,     51,      8,      3],\n",
      "        [     2,    793,    754,     24,   4497,   1219,      0,      3],\n",
      "        [     2,   6764,     18,      7,   1151,     28,    632,      3],\n",
      "        [     2,    191,  14015,   1610,      5,   8783,    685,      3],\n",
      "        [     2,   2375,     97,    439,     83,     24,     44,      3],\n",
      "        [     2,   1476,    633,     86,    608,    635,    206,      3],\n",
      "        [     2,      5,      7,     38,   3061,     48,     44,      3],\n",
      "        [     2,    390,     46,     14,   3313,   3194,   1259,      3],\n",
      "        [     2,  12527,   1746,    558,      9,     87,     47,      3],\n",
      "        [     2,   4465,     13,     34,   3377,    679,    460,      3],\n",
      "        [     2,    151,     83,    101,     83,    122,      8,      3],\n",
      "        [     2,      7,     13,     34,   3876,      4,      4,      3],\n",
      "        [     2,   5981,     13,    388,      7,     48,     44,      3],\n",
      "        [     2,    715,     39,   2818,     36,   3300,      4,      3],\n",
      "        [     2,    242,   5871,     34,     45,     73,     23,      3],\n",
      "        [     2,    561,     10,    121,     50,      4,     19,      3],\n",
      "        [     2,     10,      4,     84,   7486,    146,      4,      3],\n",
      "        [     2,     75,    249,     17,     60,    447,    187,      3],\n",
      "        [     2,     30,     12,   1664,     18,   1484,   2919,      3],\n",
      "        [     2,   5005,    332,    146,      4,      0,    685,      3]], device='cuda:0'), tensor([ 8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "         8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "         8,  8,  8,  8], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = BucketIterator.splits((train_data,test_data),sort_key=lambda x:len(x.text), sort_within_batch=True,\n",
    " repeat=False,shuffle=True,\n",
    " batch_size=32,device=DEVICE)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print (batch.text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from attention import Attention\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size,embed_size,hidden_size,output_size,num_layers=1,bidirec=False,vocab=None):\n",
    "        super(RNN,self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        if bidirec:\n",
    "            self.num_directions = 2\n",
    "        else:\n",
    "            self.num_directions = 1\n",
    "            \n",
    "        self.embed = nn.Embedding(input_size,embed_size)\n",
    "#         self.embed.weight.data.copy_(vocab.vectors)\n",
    "        self.lstm = nn.LSTM(embed_size,hidden_size,num_layers,batch_first=True,bidirectional=bidirec)\n",
    "        self.linear = nn.Linear(hidden_size*self.num_directions,output_size)\n",
    "#         self.attention = Attention(self.hidden_size*self.num_directions, method='general')\n",
    "        \n",
    "    def init_hidden(self,batch_size):\n",
    "        # (num_layers * num_directions, batch_size, hidden_size)\n",
    "        \n",
    "        hidden = Variable(torch.zeros(self.num_layers*self.num_directions,batch_size,self.hidden_size)).cuda()\n",
    "        cell = Variable(torch.zeros(self.num_layers*self.num_directions,batch_size,self.hidden_size)).cuda()\n",
    "        return hidden, cell\n",
    "\n",
    "    def forward(self,inputs, encoder_length=None):\n",
    "        \"\"\"\n",
    "        inputs : B,T\n",
    "        \"\"\"\n",
    "        embed = self.embed(inputs) # word vector indexing\n",
    "        hidden, cell = self.init_hidden(inputs.size(0)) # initial hidden,cell\n",
    "        \n",
    "        output, h = self.lstm(embed,(hidden,cell)) # output : Batch,Time,Hidden (32, 9, 200)\n",
    "        hidden, cell = h # hidden : L, B, H\n",
    "        \n",
    "        hidden = hidden[-self.num_directions:] # (num_directions,B,H)\n",
    "        hidden = torch.cat([h for h in hidden],1) #.unsqueeze(0) # (1,B,2H) (1, 32, 200)\n",
    "        \n",
    "#         print (output.size(), hidden.size(), len(encoder_length))\n",
    "#         print (encoder_length, type(encoder_length[0]))\n",
    "        \n",
    "        # context : Batch, 1, Hidden*bi\n",
    "#         context, attn_weight = self.attention(hidden.transpose(0,1), output, encoder_length, return_weight=True)\n",
    "#         print (context.size())\n",
    "        \n",
    "        # Many-to-One\n",
    "        output = self.linear(hidden) # last hidden\n",
    "        output = output.squeeze(1)\n",
    "#         print (output.size())\n",
    "        \n",
    "#         return F.softmax(output)\n",
    "        return F.log_softmax(output, dim=1)#, attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH = 10\n",
    "BATCH_SIZE = 32\n",
    "EMBED = 200\n",
    "KERNEL_DIM = 100\n",
    "LR = 0.001\n",
    "output_size = 2\n",
    "\n",
    "# model = CNNClassifier(len(TEXT.vocab), EMBED, 1, KERNEL_DIM, KERNEL_SIZES)\n",
    "model = RNN(len(TEXT.vocab), EMBED, KERNEL_DIM, output_size, num_layers=2, bidirec=True, vocab=TEXT.vocab)\n",
    "\n",
    "# loss_function = nn.BCELoss()\n",
    "# loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = nn.NLLLoss()\n",
    "# loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5], gamma=0.1)\n",
    "\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('./model/nsmc.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0 mean_loss : 0.697 , lr : 0.00100\n",
      "epoch : 0 mean_loss : 0.470 , lr : 0.00100\n",
      "epoch : 0 mean_loss : 0.371 , lr : 0.00100\n",
      "epoch : 0 mean_loss : 0.344 , lr : 0.00100\n",
      "epoch : 0 mean_loss : 0.324 , lr : 0.00100\n",
      "epoch : 1 mean_loss : 0.281 , lr : 0.00100\n",
      "epoch : 1 mean_loss : 0.262 , lr : 0.00100\n",
      "epoch : 1 mean_loss : 0.264 , lr : 0.00100\n",
      "epoch : 1 mean_loss : 0.261 , lr : 0.00100\n",
      "epoch : 1 mean_loss : 0.258 , lr : 0.00100\n",
      "epoch : 2 mean_loss : 0.137 , lr : 0.00100\n",
      "epoch : 2 mean_loss : 0.190 , lr : 0.00100\n",
      "epoch : 2 mean_loss : 0.198 , lr : 0.00100\n",
      "epoch : 2 mean_loss : 0.195 , lr : 0.00100\n",
      "epoch : 2 mean_loss : 0.197 , lr : 0.00100\n",
      "epoch : 3 mean_loss : 0.319 , lr : 0.00100\n",
      "epoch : 3 mean_loss : 0.121 , lr : 0.00100\n",
      "epoch : 3 mean_loss : 0.134 , lr : 0.00100\n",
      "epoch : 3 mean_loss : 0.138 , lr : 0.00100\n",
      "epoch : 3 mean_loss : 0.145 , lr : 0.00100\n",
      "epoch : 4 mean_loss : 0.057 , lr : 0.00100\n",
      "epoch : 4 mean_loss : 0.076 , lr : 0.00100\n",
      "epoch : 4 mean_loss : 0.085 , lr : 0.00100\n",
      "epoch : 4 mean_loss : 0.095 , lr : 0.00100\n",
      "epoch : 4 mean_loss : 0.100 , lr : 0.00100\n",
      "epoch : 5 mean_loss : 0.028 , lr : 0.00010\n",
      "epoch : 5 mean_loss : 0.050 , lr : 0.00010\n",
      "epoch : 5 mean_loss : 0.042 , lr : 0.00010\n",
      "epoch : 5 mean_loss : 0.041 , lr : 0.00010\n",
      "epoch : 5 mean_loss : 0.037 , lr : 0.00010\n",
      "epoch : 6 mean_loss : 0.170 , lr : 0.00010\n",
      "epoch : 6 mean_loss : 0.025 , lr : 0.00010\n",
      "epoch : 6 mean_loss : 0.025 , lr : 0.00010\n",
      "epoch : 6 mean_loss : 0.025 , lr : 0.00010\n",
      "epoch : 6 mean_loss : 0.026 , lr : 0.00010\n",
      "epoch : 7 mean_loss : 0.074 , lr : 0.00010\n",
      "epoch : 7 mean_loss : 0.017 , lr : 0.00010\n",
      "epoch : 7 mean_loss : 0.017 , lr : 0.00010\n",
      "epoch : 7 mean_loss : 0.017 , lr : 0.00010\n",
      "epoch : 7 mean_loss : 0.017 , lr : 0.00010\n",
      "epoch : 8 mean_loss : 0.002 , lr : 0.00010\n",
      "epoch : 8 mean_loss : 0.012 , lr : 0.00010\n",
      "epoch : 8 mean_loss : 0.013 , lr : 0.00010\n",
      "epoch : 8 mean_loss : 0.013 , lr : 0.00010\n",
      "epoch : 8 mean_loss : 0.014 , lr : 0.00010\n",
      "epoch : 9 mean_loss : 0.046 , lr : 0.00010\n",
      "epoch : 9 mean_loss : 0.011 , lr : 0.00010\n",
      "epoch : 9 mean_loss : 0.011 , lr : 0.00010\n",
      "epoch : 9 mean_loss : 0.010 , lr : 0.00010\n",
      "epoch : 9 mean_loss : 0.012 , lr : 0.00010\n"
     ]
    }
   ],
   "source": [
    "### train\n",
    "model.train()\n",
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    scheduler.step()\n",
    "    for i,batch in enumerate(train_loader):\n",
    "        inputs, lengths = batch.text\n",
    "        targets = batch.label#.float()\n",
    "        # print (inputs.size())\n",
    "        if USE_CUDA:\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        \n",
    "        model.zero_grad()\n",
    "        # model.batch_size = inputs.size()[0] # N, sent\n",
    "        # model.hidden = model.init_hidden()\n",
    "        # preds = model(inputs.t()) # sent, N\n",
    "#         preds, attn_weights = model(inputs, encoder_length=lengths.tolist())\n",
    "        preds = model(inputs, encoder_length=lengths.tolist())\n",
    "        # print (preds, targets)\n",
    "        # print (preds.squeeze(1).size(), targets.size())\n",
    "#         loss = loss_function(preds.squeeze(1),targets)\n",
    "        loss = loss_function(preds,targets)\n",
    "        losses.append(loss.item()) #data[0])\n",
    "        # exit()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         break\n",
    "        if i % 1000 == 0:\n",
    "            print(\"epoch : %d mean_loss : %.3f , lr : %.5f\" % (epoch,np.mean(losses), scheduler.get_lr()[0]))\n",
    "            losses=[]\n",
    "#             torch.save(model.state_dict(), './model/rnn_news_text%03d.pth'%(i//1000))\n",
    "#     break\n",
    "torch.save(model.state_dict(), './model/nsmc_lm.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 86.378%\n"
     ]
    }
   ],
   "source": [
    "### evaluate\n",
    "model.eval()\n",
    "num_hit=0\n",
    "for i,batch in enumerate(test_loader):\n",
    "    inputs, lengths = batch.text\n",
    "    targets = batch.label#.float()\n",
    "    \n",
    "#     print ('\\n',inputs.size())\n",
    "#     print (inputs)\n",
    "#     break\n",
    "    if USE_CUDA:\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "#     inputs = inputs\n",
    "#     targets = ((targets-1)/4).round()\n",
    "#     output, attn_weights = model(inputs, lengths)\n",
    "    output = model(inputs, lengths)\n",
    "#     output = output.squeeze(1)\n",
    "    preds = output.max(1, keepdim=True)[1]\n",
    "#     print (preds.size())\n",
    "#     print (preds)\n",
    "#     print (targets)\n",
    "#     preds = preds*5\n",
    "#     preds = preds.round()\n",
    "    num_hit+=torch.eq(preds.squeeze(),targets.squeeze()).sum().item() #data[0]\n",
    "\n",
    "print('test accuracy: %.3f%%'%(num_hit/len(test_data)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "개별적인 장면이 좋았다. \u001b[1;01;36m긍정\u001b[0m\n",
      "\n",
      "존맛탱! \u001b[1;01;36m긍정\u001b[0m\n",
      "\n",
      "헐 진짜 개별로다.. \u001b[1;01;36m긍정\u001b[0m\n",
      "\n",
      "진짜 너무 재밌는 영화다 오랜만에 \u001b[1;01;36m긍정\u001b[0m\n",
      "\n",
      "오..이건 진짜 봐야함 \u001b[1;01;36m긍정\u001b[0m\n",
      "\n",
      "진짜 쓰레기 같은 영화 \u001b[1;01;31m부정\u001b[0m\n",
      "\n",
      "노잼 \u001b[1;01;31m부정\u001b[0m\n",
      "\n",
      "존잼 \u001b[1;01;36m긍정\u001b[0m\n",
      "\n",
      "꾸울잼 \u001b[1;01;36m긍정\u001b[0m\n",
      "\n",
      "핵노잼 \u001b[1;01;31m부정\u001b[0m\n",
      "\n",
      "또 보고싶다 \u001b[1;01;36m긍정\u001b[0m\n",
      "\n",
      "꼬옥 봐야한다.. 진짜.. \u001b[1;01;36m긍정\u001b[0m\n",
      "\n",
      "나만 보기 아깝다 \u001b[1;01;31m부정\u001b[0m\n",
      "\n",
      "돈이 아깝다 \u001b[1;01;31m부정\u001b[0m\n",
      "\n",
      "나만 보기 억울하다 \u001b[1;01;31m부정\u001b[0m\n",
      "\n",
      "나만 당할 수 없다 \u001b[1;01;36m긍정\u001b[0m\n",
      "\n",
      "너도 봐야한다 \u001b[1;01;36m긍정\u001b[0m\n",
      "\n",
      "혼자 본게 정말 후회된다. 이건 꼭 같이 봐야한다. \u001b[1;01;36m긍정\u001b[0m\n",
      "\n",
      "재미없어요... \u001b[1;01;31m부정\u001b[0m\n",
      "\n",
      "꾸르르르르르르잼 \u001b[1;01;36m긍정\u001b[0m\n",
      "\n",
      "꾸르르르잼 \u001b[1;01;36m긍정\u001b[0m\n",
      "\n",
      "꾸르잼 \u001b[1;01;36m긍정\u001b[0m\n",
      "\n",
      "이 영화를 보고 암이 나았습니다. \u001b[1;01;36m긍정\u001b[0m\n",
      "\n",
      "['0', '1']\n"
     ]
    }
   ],
   "source": [
    "### test\n",
    "test_inputs = [\"개별적인 장면이 좋았다.\", \"존맛탱!\", \"헐 진짜 개별로다..\", \"진짜 너무 재밌는 영화다 오랜만에\",\"오..이건 진짜 봐야함\", \"진짜 쓰레기 같은 영화\",\"노잼\",\"존잼\",\"꾸울잼\",\"핵노잼\",'또 보고싶다', '꼬옥 봐야한다.. 진짜..', '나만 보기 아깝다', '돈이 아깝다', '나만 보기 억울하다', '나만 당할 수 없다', '너도 봐야한다', '혼자 본게 정말 후회된다. 이건 꼭 같이 봐야한다.', '재미없어요...', '꾸르르르르르르잼', '꾸르르르잼', '꾸르잼', '이 영화를 보고 암이 나았습니다.']\n",
    "\n",
    "for test_input in test_inputs:\n",
    "    tokenized = tagger.morphs(test_input)\n",
    "    length = len(tokenized)\n",
    "    tokenized = pad_under_five(tokenized)\n",
    "    input_, lengths = TEXT.numericalize(([tokenized], length), device=DEVICE)\n",
    "    if USE_CUDA: input_ = input_.cuda()\n",
    "    \n",
    "#     output, attn_weights = model(input_, [lengths.tolist()])\n",
    "    output = model(input_, [lengths.tolist()])\n",
    "#     print (tokenized)\n",
    "#     print (attn_weights.squeeze().data.cpu().numpy())\n",
    "#     print (output)\n",
    "    prediction = output.max(1, keepdim=True)[1]\n",
    "#     print (prediction)\n",
    "    if prediction[0][0] == 1:\n",
    "        print(test_input,\"\\033[1;01;36m\" + '긍정' + \"\\033[0m\")\n",
    "    else:\n",
    "        print(test_input,\"\\033[1;01;31m\" + '부정' + \"\\033[0m\")\n",
    "    print ()\n",
    "    \n",
    "print (LABEL.vocab.itos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), './model/rnn_self_text.pth')\n",
    "torch.save(TEXT.vocab, './model/news_field.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print (LABEL.numericalize(['스포츠']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocaburay = torch.load('./model/vocab.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '.', '이', '다', '을', '는', '의', '에', '은']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    49],\n",
       "        [   357],\n",
       "        [    47],\n",
       "        [   121],\n",
       "        [     0],\n",
       "        [  2299],\n",
       "        [  3103],\n",
       "        [ 22138],\n",
       "        [  2646],\n",
       "        [   631],\n",
       "        [  7827],\n",
       "        [ 16320],\n",
       "        [     8],\n",
       "        [     0],\n",
       "        [  8500],\n",
       "        [    56],\n",
       "        [    17],\n",
       "        [    53],\n",
       "        [     0],\n",
       "        [  1078],\n",
       "        [ 22138],\n",
       "        [  1207],\n",
       "        [  2062],\n",
       "        [     3],\n",
       "        [  6831],\n",
       "        [     0],\n",
       "        [    68],\n",
       "        [     3],\n",
       "        [  1207],\n",
       "        [  9949],\n",
       "        [  1207],\n",
       "        [     0],\n",
       "        [  1672],\n",
       "        [    14],\n",
       "        [    79],\n",
       "        [   854],\n",
       "        [     0],\n",
       "        [  2642],\n",
       "        [   976],\n",
       "        [   141],\n",
       "        [     0],\n",
       "        [ 46748],\n",
       "        [  7148],\n",
       "        [     0],\n",
       "        [  3740],\n",
       "        [     3],\n",
       "        [     0],\n",
       "        [    36],\n",
       "        [  4036],\n",
       "        [    11],\n",
       "        [     0],\n",
       "        [   548],\n",
       "        [  8438],\n",
       "        [    17],\n",
       "        [     4],\n",
       "        [     2],\n",
       "        [    12],\n",
       "        [    13],\n",
       "        [    36],\n",
       "        [  4036],\n",
       "        [     0],\n",
       "        [     3],\n",
       "        [   976],\n",
       "        [    11],\n",
       "        [     0],\n",
       "        [   107],\n",
       "        [    15],\n",
       "        [     0],\n",
       "        [  2642],\n",
       "        [   976],\n",
       "        [   141],\n",
       "        [    32],\n",
       "        [     0],\n",
       "        [ 46748],\n",
       "        [  7148],\n",
       "        [     3],\n",
       "        [     0],\n",
       "        [   187],\n",
       "        [    25],\n",
       "        [     0],\n",
       "        [   318],\n",
       "        [   884],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [  6710],\n",
       "        [   371],\n",
       "        [    35],\n",
       "        [    74],\n",
       "        [     0],\n",
       "        [   935],\n",
       "        [   935],\n",
       "        [     0],\n",
       "        [     4],\n",
       "        [ 32191],\n",
       "        [     0],\n",
       "        [    91],\n",
       "        [   360],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [ 14807],\n",
       "        [  4585],\n",
       "        [     0],\n",
       "        [  8047],\n",
       "        [   378],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [  1531],\n",
       "        [    15],\n",
       "        [     0],\n",
       "        [    18],\n",
       "        [     4],\n",
       "        [     2],\n",
       "        [    12],\n",
       "        [    13],\n",
       "        [ 46748],\n",
       "        [  7148],\n",
       "        [     9],\n",
       "        [     0],\n",
       "        [    47],\n",
       "        [   121],\n",
       "        [    29],\n",
       "        [     0],\n",
       "        [    89],\n",
       "        [   301],\n",
       "        [     7],\n",
       "        [     0],\n",
       "        [  1728],\n",
       "        [ 25016],\n",
       "        [   667],\n",
       "        [  1313],\n",
       "        [  6871],\n",
       "        [   278],\n",
       "        [   187],\n",
       "        [   238],\n",
       "        [  1207],\n",
       "        [    23],\n",
       "        [     0],\n",
       "        [     0],\n",
       "        [     0],\n",
       "        [    24],\n",
       "        [     0],\n",
       "        [   429],\n",
       "        [   691],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [  2839],\n",
       "        [    36],\n",
       "        [     0],\n",
       "        [     0],\n",
       "        [  2642],\n",
       "        [   976],\n",
       "        [   141],\n",
       "        [     7],\n",
       "        [     0],\n",
       "        [ 10290],\n",
       "        [   691],\n",
       "        [ 23106],\n",
       "        [    25],\n",
       "        [     0],\n",
       "        [    30],\n",
       "        [  1273],\n",
       "        [     0],\n",
       "        [    52],\n",
       "        [    31],\n",
       "        [   193],\n",
       "        [     7],\n",
       "        [     0],\n",
       "        [ 10115],\n",
       "        [  2655],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [  2085],\n",
       "        [   546],\n",
       "        [  1456],\n",
       "        [    10],\n",
       "        [    37],\n",
       "        [     0],\n",
       "        [    75],\n",
       "        [     4],\n",
       "        [     0],\n",
       "        [    74],\n",
       "        [     0],\n",
       "        [     0],\n",
       "        [    49],\n",
       "        [   357],\n",
       "        [    49],\n",
       "        [    49],\n",
       "        [    31],\n",
       "        [     0],\n",
       "        [  2183],\n",
       "        [     3],\n",
       "        [  9269],\n",
       "        [     0],\n",
       "        [ 22138],\n",
       "        [  2646],\n",
       "        [   631],\n",
       "        [  7827],\n",
       "        [ 16320],\n",
       "        [  4221],\n",
       "        [    30],\n",
       "        [     0],\n",
       "        [  8500],\n",
       "        [    56],\n",
       "        [    10],\n",
       "        [   999],\n",
       "        [     0],\n",
       "        [    17],\n",
       "        [    30],\n",
       "        [    40],\n",
       "        [     0],\n",
       "        [     0],\n",
       "        [    88],\n",
       "        [   516],\n",
       "        [     0],\n",
       "        [    37],\n",
       "        [     0],\n",
       "        [   437],\n",
       "        [   635],\n",
       "        [     0],\n",
       "        [ 10115],\n",
       "        [  2655],\n",
       "        [    10],\n",
       "        [    30],\n",
       "        [     0],\n",
       "        [   399],\n",
       "        [    10],\n",
       "        [    37],\n",
       "        [     0],\n",
       "        [    75],\n",
       "        [     4],\n",
       "        [     0],\n",
       "        [    15],\n",
       "        [     0],\n",
       "        [  2893],\n",
       "        [ 29267],\n",
       "        [     4],\n",
       "        [     2],\n",
       "        [    12],\n",
       "        [    13],\n",
       "        [     3],\n",
       "        [    45],\n",
       "        [     0],\n",
       "        [     0],\n",
       "        [    16],\n",
       "        [  1672],\n",
       "        [     0],\n",
       "        [  2611],\n",
       "        [   106],\n",
       "        [     0],\n",
       "        [   548],\n",
       "        [    41],\n",
       "        [    25],\n",
       "        [     0],\n",
       "        [  2299],\n",
       "        [  3103],\n",
       "        [   631],\n",
       "        [  7827],\n",
       "        [ 16320],\n",
       "        [     8],\n",
       "        [     0],\n",
       "        [  8500],\n",
       "        [    56],\n",
       "        [    16],\n",
       "        [     0],\n",
       "        [    26],\n",
       "        [     9],\n",
       "        [     0],\n",
       "        [   224],\n",
       "        [     0],\n",
       "        [  3024],\n",
       "        [  6027],\n",
       "        [     3],\n",
       "        [    39],\n",
       "        [     4],\n",
       "        [     0],\n",
       "        [    74],\n",
       "        [     0],\n",
       "        [     0],\n",
       "        [    16],\n",
       "        [  1672],\n",
       "        [     0],\n",
       "        [   856],\n",
       "        [    21],\n",
       "        [     7],\n",
       "        [     0],\n",
       "        [  8448],\n",
       "        [    65],\n",
       "        [    27],\n",
       "        [     0],\n",
       "        [  2623],\n",
       "        [   119],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [    14],\n",
       "        [ 61962],\n",
       "        [     0],\n",
       "        [   305],\n",
       "        [     8],\n",
       "        [     0],\n",
       "        [   193],\n",
       "        [  1353],\n",
       "        [    10],\n",
       "        [    84],\n",
       "        [     4],\n",
       "        [     0],\n",
       "        [    15],\n",
       "        [     0],\n",
       "        [    56],\n",
       "        [    17],\n",
       "        [     4],\n",
       "        [     2],\n",
       "        [    12],\n",
       "        [    13],\n",
       "        [ 46748],\n",
       "        [  7148],\n",
       "        [     7],\n",
       "        [     0],\n",
       "        [    36],\n",
       "        [  4036],\n",
       "        [     0],\n",
       "        [   548],\n",
       "        [  8438],\n",
       "        [     0],\n",
       "        [  1353],\n",
       "        [   670],\n",
       "        [     0],\n",
       "        [  2642],\n",
       "        [   976],\n",
       "        [   141],\n",
       "        [     6],\n",
       "        [     0],\n",
       "        [    89],\n",
       "        [   301],\n",
       "        [     7],\n",
       "        [     0],\n",
       "        [     8],\n",
       "        [  1207],\n",
       "        [   188],\n",
       "        [     8],\n",
       "        [  1207],\n",
       "        [    11],\n",
       "        [     0],\n",
       "        [  2839],\n",
       "        [    36],\n",
       "        [     0],\n",
       "        [ 15648],\n",
       "        [  1182],\n",
       "        [  7148],\n",
       "        [     0],\n",
       "        [    91],\n",
       "        [   360],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [  7187],\n",
       "        [     4],\n",
       "        [     2],\n",
       "        [     0],\n",
       "        [    43],\n",
       "        [     6],\n",
       "        [     0],\n",
       "        [     0],\n",
       "        [   132],\n",
       "        [     9],\n",
       "        [     0],\n",
       "        [   153],\n",
       "        [     7],\n",
       "        [     0],\n",
       "        [  1390],\n",
       "        [  7715],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [    64],\n",
       "        [    15],\n",
       "        [     0],\n",
       "        [ 46748],\n",
       "        [  7148],\n",
       "        [     3],\n",
       "        [     0],\n",
       "        [     8],\n",
       "        [  1207],\n",
       "        [   188],\n",
       "        [     8],\n",
       "        [  1207],\n",
       "        [     8],\n",
       "        [     0],\n",
       "        [   411],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [   631],\n",
       "        [  7148],\n",
       "        [     0],\n",
       "        [    26],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [   239],\n",
       "        [    37],\n",
       "        [     0],\n",
       "        [    75],\n",
       "        [     4],\n",
       "        [     0],\n",
       "        [    74],\n",
       "        [     0],\n",
       "        [ 46748],\n",
       "        [  7148],\n",
       "        [     7],\n",
       "        [     0],\n",
       "        [    36],\n",
       "        [  4036],\n",
       "        [     0],\n",
       "        [   548],\n",
       "        [  8438],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [    33],\n",
       "        [    30],\n",
       "        [    10],\n",
       "        [    30],\n",
       "        [     0],\n",
       "        [   399],\n",
       "        [    17],\n",
       "        [     4],\n",
       "        [     6],\n",
       "        [     0],\n",
       "        [   436],\n",
       "        [     7],\n",
       "        [     0],\n",
       "        [   411],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [   631],\n",
       "        [ 27980],\n",
       "        [     4],\n",
       "        [     2],\n",
       "        [     0],\n",
       "        [  2642],\n",
       "        [   976],\n",
       "        [   141],\n",
       "        [     6],\n",
       "        [     0],\n",
       "        [     0],\n",
       "        [    30],\n",
       "        [  1273],\n",
       "        [    91],\n",
       "        [  4221],\n",
       "        [    30],\n",
       "        [     0],\n",
       "        [ 46748],\n",
       "        [  7148],\n",
       "        [    27],\n",
       "        [     0],\n",
       "        [   148],\n",
       "        [     0],\n",
       "        [  5343],\n",
       "        [    25],\n",
       "        [    43],\n",
       "        [ 15387],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [ 11019],\n",
       "        [  5981],\n",
       "        [    17],\n",
       "        [     6],\n",
       "        [   160],\n",
       "        [     0],\n",
       "        [     0],\n",
       "        [ 46748],\n",
       "        [  7148],\n",
       "        [     3],\n",
       "        [     0],\n",
       "        [   198],\n",
       "        [    56],\n",
       "        [     0],\n",
       "        [  4344],\n",
       "        [  2655],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [    10],\n",
       "        [    30],\n",
       "        [     0],\n",
       "        [    46],\n",
       "        [    15],\n",
       "        [     0],\n",
       "        [ 11019],\n",
       "        [  5981],\n",
       "        [     8],\n",
       "        [     0],\n",
       "        [   884],\n",
       "        [    10],\n",
       "        [     6],\n",
       "        [     0],\n",
       "        [    34],\n",
       "        [     0],\n",
       "        [    69],\n",
       "        [  4614],\n",
       "        [    16],\n",
       "        [     0],\n",
       "        [  3308],\n",
       "        [ 35204],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [    71],\n",
       "        [   159],\n",
       "        [     4],\n",
       "        [     0],\n",
       "        [    74],\n",
       "        [     0],\n",
       "        [     0],\n",
       "        [ 46748],\n",
       "        [  7148],\n",
       "        [     8],\n",
       "        [    37],\n",
       "        [     0],\n",
       "        [   787],\n",
       "        [   238],\n",
       "        [    14],\n",
       "        [     0],\n",
       "        [   143],\n",
       "        [     0],\n",
       "        [    85],\n",
       "        [  4221],\n",
       "        [    30],\n",
       "        [     0],\n",
       "        [ 11019],\n",
       "        [  5981],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [    98],\n",
       "        [   675],\n",
       "        [    10],\n",
       "        [    89],\n",
       "        [    15],\n",
       "        [     0],\n",
       "        [    17],\n",
       "        [     6],\n",
       "        [   160],\n",
       "        [     0],\n",
       "        [     0],\n",
       "        [    68],\n",
       "        [  1353],\n",
       "        [     0],\n",
       "        [  5774],\n",
       "        [   106],\n",
       "        [    14],\n",
       "        [     0],\n",
       "        [    50],\n",
       "        [     6],\n",
       "        [    98],\n",
       "        [     0],\n",
       "        [     3],\n",
       "        [   141],\n",
       "        [    15],\n",
       "        [     0],\n",
       "        [  2893],\n",
       "        [ 29267],\n",
       "        [     4],\n",
       "        [     2],\n",
       "        [     0],\n",
       "        [    36],\n",
       "        [  4036],\n",
       "        [    14],\n",
       "        [     0],\n",
       "        [    68],\n",
       "        [   298],\n",
       "        [   141],\n",
       "        [     0],\n",
       "        [    29],\n",
       "        [   118],\n",
       "        [    28],\n",
       "        [    33],\n",
       "        [     0],\n",
       "        [ 11019],\n",
       "        [  5981],\n",
       "        [     0],\n",
       "        [    98],\n",
       "        [   675],\n",
       "        [     3],\n",
       "        [    39],\n",
       "        [     4],\n",
       "        [     6],\n",
       "        [     0],\n",
       "        [    26],\n",
       "        [     3],\n",
       "        [     4],\n",
       "        [     2],\n",
       "        [     0],\n",
       "        [  2642],\n",
       "        [   976],\n",
       "        [   141],\n",
       "        [     6],\n",
       "        [     0],\n",
       "        [   670],\n",
       "        [    65],\n",
       "        [     0],\n",
       "        [ 16406],\n",
       "        [  6979],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [ 46748],\n",
       "        [  7148],\n",
       "        [     0],\n",
       "        [   191],\n",
       "        [     8],\n",
       "        [   187],\n",
       "        [     0],\n",
       "        [  1196],\n",
       "        [  1456],\n",
       "        [    10],\n",
       "        [    15],\n",
       "        [     0],\n",
       "        [    18],\n",
       "        [     4],\n",
       "        [    15],\n",
       "        [     0],\n",
       "        [  2893],\n",
       "        [  9132],\n",
       "        [    35],\n",
       "        [    22],\n",
       "        [     0],\n",
       "        [    17],\n",
       "        [     4],\n",
       "        [     2],\n",
       "        [     0],\n",
       "        [    43],\n",
       "        [     6],\n",
       "        [     0],\n",
       "        [     0],\n",
       "        [    23],\n",
       "        [ 16406],\n",
       "        [  6979],\n",
       "        [     9],\n",
       "        [    24],\n",
       "        [     0],\n",
       "        [ 46748],\n",
       "        [  7148],\n",
       "        [     0],\n",
       "        [   246],\n",
       "        [  3308],\n",
       "        [   578],\n",
       "        [     3],\n",
       "        [     0],\n",
       "        [   118],\n",
       "        [   679],\n",
       "        [    16],\n",
       "        [     0],\n",
       "        [    26],\n",
       "        [     3],\n",
       "        [   141],\n",
       "        [     0],\n",
       "        [  3308],\n",
       "        [   107],\n",
       "        [     0],\n",
       "        [ 46748],\n",
       "        [  7148],\n",
       "        [     0],\n",
       "        [   246],\n",
       "        [  3308],\n",
       "        [   578],\n",
       "        [     3],\n",
       "        [     0],\n",
       "        [   526],\n",
       "        [    15],\n",
       "        [     0],\n",
       "        [    18],\n",
       "        [ 23106],\n",
       "        [    74],\n",
       "        [     0],\n",
       "        [     0],\n",
       "        [    45],\n",
       "        [     0],\n",
       "        [    37],\n",
       "        [     0],\n",
       "        [   451],\n",
       "        [     3],\n",
       "        [    15],\n",
       "        [     0],\n",
       "        [    18],\n",
       "        [     6],\n",
       "        [    30],\n",
       "        [     0],\n",
       "        [   228],\n",
       "        [     0],\n",
       "        [  3308],\n",
       "        [ 32191],\n",
       "        [     4],\n",
       "        [     0],\n",
       "        [    15],\n",
       "        [     0],\n",
       "        [  2893],\n",
       "        [ 29267],\n",
       "        [     4],\n",
       "        [     2],\n",
       "        [     0],\n",
       "        [   107],\n",
       "        [     0],\n",
       "        [   548],\n",
       "        [    41],\n",
       "        [     6],\n",
       "        [     0],\n",
       "        [  2299],\n",
       "        [  3103],\n",
       "        [   631],\n",
       "        [  7827],\n",
       "        [ 16320],\n",
       "        [     0],\n",
       "        [  1353],\n",
       "        [   670],\n",
       "        [     0],\n",
       "        [  2007],\n",
       "        [    56],\n",
       "        [    28],\n",
       "        [  1273],\n",
       "        [     8],\n",
       "        [     0],\n",
       "        [  1353],\n",
       "        [    54],\n",
       "        [    17],\n",
       "        [     6],\n",
       "        [   160],\n",
       "        [     0],\n",
       "        [     0],\n",
       "        [     3],\n",
       "        [     0],\n",
       "        [   198],\n",
       "        [  1390],\n",
       "        [     3],\n",
       "        [     0],\n",
       "        [   239],\n",
       "        [   999],\n",
       "        [    30],\n",
       "        [    89],\n",
       "        [     0],\n",
       "        [    56],\n",
       "        [  1672],\n",
       "        [     0],\n",
       "        [   935],\n",
       "        [    30],\n",
       "        [     8],\n",
       "        [   187],\n",
       "        [     0],\n",
       "        [   630],\n",
       "        [     0],\n",
       "        [    47],\n",
       "        [    49],\n",
       "        [    40],\n",
       "        [    67],\n",
       "        [    52],\n",
       "        [    67],\n",
       "        [   357],\n",
       "        [   207],\n",
       "        [  1413],\n",
       "        [    23],\n",
       "        [   632],\n",
       "        [     0],\n",
       "        [    47],\n",
       "        [   119],\n",
       "        [    67],\n",
       "        [   412],\n",
       "        [    40],\n",
       "        [    65],\n",
       "        [    24],\n",
       "        [     7],\n",
       "        [     0],\n",
       "        [   670],\n",
       "        [    65],\n",
       "        [  2007],\n",
       "        [     3],\n",
       "        [     0],\n",
       "        [   886],\n",
       "        [    30],\n",
       "        [    17],\n",
       "        [     4],\n",
       "        [     2],\n",
       "        [     0],\n",
       "        [     3],\n",
       "        [     0],\n",
       "        [ 16406],\n",
       "        [  6979],\n",
       "        [   188],\n",
       "        [     0],\n",
       "        [   297],\n",
       "        [   409],\n",
       "        [    33],\n",
       "        [     0],\n",
       "        [    79],\n",
       "        [  2839],\n",
       "        [  7154],\n",
       "        [     0],\n",
       "        [   104],\n",
       "        [  1116],\n",
       "        [    22],\n",
       "        [     0],\n",
       "        [   198],\n",
       "        [   238],\n",
       "        [     0],\n",
       "        [    47],\n",
       "        [   357],\n",
       "        [   357],\n",
       "        [   357],\n",
       "        [   207],\n",
       "        [  1413],\n",
       "        [    11],\n",
       "        [     0],\n",
       "        [   670],\n",
       "        [    65],\n",
       "        [    36],\n",
       "        [     0],\n",
       "        [   106],\n",
       "        [   155],\n",
       "        [    14],\n",
       "        [     0],\n",
       "        [    75],\n",
       "        [     4],\n",
       "        [     2],\n",
       "        [     0],\n",
       "        [  2642],\n",
       "        [   976],\n",
       "        [   141],\n",
       "        [     6],\n",
       "        [     0],\n",
       "        [    89],\n",
       "        [   301],\n",
       "        [     7],\n",
       "        [     0],\n",
       "        [   411],\n",
       "        [     3],\n",
       "        [     0],\n",
       "        [  8047],\n",
       "        [   378],\n",
       "        [     3],\n",
       "        [     0],\n",
       "        [    48],\n",
       "        [    89],\n",
       "        [     0],\n",
       "        [     0],\n",
       "        [ 46748],\n",
       "        [  7148],\n",
       "        [     7],\n",
       "        [     0],\n",
       "        [  1890],\n",
       "        [  2074],\n",
       "        [ 23106],\n",
       "        [    25],\n",
       "        [     0],\n",
       "        [   411],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [ 22747],\n",
       "        [   155],\n",
       "        [    16],\n",
       "        [     4],\n",
       "        [     0],\n",
       "        [    74],\n",
       "        [     0],\n",
       "        [    37],\n",
       "        [   118],\n",
       "        [   506],\n",
       "        [     5],\n",
       "        [     0],\n",
       "        [   104],\n",
       "        [ 27980],\n",
       "        [     4],\n",
       "        [     2],\n",
       "        [     0],\n",
       "        [     3],\n",
       "        [  3294],\n",
       "        [  3024],\n",
       "        [     0],\n",
       "        [    35],\n",
       "        [    89],\n",
       "        [     0],\n",
       "        [ 12247],\n",
       "        [  3531],\n",
       "        [ 18913],\n",
       "        [    47],\n",
       "        [   357],\n",
       "        [   357],\n",
       "        [     0],\n",
       "        [  4584],\n",
       "        [  1302],\n",
       "        [    13],\n",
       "        [  8043],\n",
       "        [     2],\n",
       "        [  3531],\n",
       "        [ 10759],\n",
       "        [     2],\n",
       "        [  1669],\n",
       "        [ 10116]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (vocaburay.itos[:10])\n",
    "TEST = Field(tokenize=tagger.morphs,lower=True,include_lengths=False,batch_first=True,preprocessing=pad_under_five)\n",
    "TEST.build_vocab()\n",
    "TEST.vocab = vocaburay\n",
    "TEST.numericalize(test_inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
