{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서 (Tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import torchvision.datasets as dsets\n",
    "# import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "# 파이토치 0.4.0.부터는 Variable 선언을 해주지 않아도 된다!!\n",
    "# placeholder, variable처럼 분할해서 작성하지 않아도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  1.5846e+29, -3.4082e-34],\n",
      "        [-3.6902e+19,  5.5828e+37,  4.5817e-41],\n",
      "        [ 5.5892e+37,  4.5817e-41,  5.5893e+37],\n",
      "        [ 4.5817e-41,  0.0000e+00,  4.2039e-45],\n",
      "        [ 0.0000e+00,  0.0000e+00,  7.0065e-45]])\n"
     ]
    }
   ],
   "source": [
    "# 초기화되지 않은 5 x 3 행렬\n",
    "x = torch.empty(5,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3128, 0.5602, 0.2848],\n",
      "        [0.6094, 0.0136, 0.6383],\n",
      "        [0.9768, 0.7672, 0.7755],\n",
      "        [0.1396, 0.9115, 0.1370],\n",
      "        [0.2160, 0.3598, 0.0987]])\n",
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 랜덤으로 초기화된 5 x 3행렬\n",
    "x = torch.rand(5, 3)\n",
    "# x 에 대한 설명 : 0~1 범위의 균등분포에서 추출한 값들\n",
    "print(x)\n",
    "# 0으로 채워지고 long 데이터 타입을 가지는 행렬\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 이미 존재하는 텐서를 기반으로 새로운 텐서 생성가능\n",
    "# 입력 텐서의 type들이 사용자에 의해 새롭게 제공되지 않는 이상 기존의 값들을 사용\n",
    "x = x.new_ones(5, 3, dtype = torch.double) # 1로 구성\n",
    "print(x) # new_* methods take in sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5975,  0.3510, -1.2507],\n",
      "        [-0.5902, -1.3408,  1.8553],\n",
      "        [-0.3964, -1.2809,  1.3245],\n",
      "        [-0.4634, -0.0044, -1.0668],\n",
      "        [-1.0602, -0.1396, -1.5950]])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn_like(x, dtype = torch.float) # override dtype (randn : random normal)\n",
    "# x 에 대한 설명 : 평균이 0, 분산이 1인 표준정규분포에서 추출한 값들을 사용, x를 이용해서 5,3 size\n",
    "print(x)      # result has the same size\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연산 (Operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2973,  0.4552, -0.3719],\n",
      "        [ 0.3067, -0.4941,  2.4571],\n",
      "        [ 0.5993, -0.6067,  1.4955],\n",
      "        [ 0.1476,  0.1020, -0.8184],\n",
      "        [-0.3789,  0.5761, -1.0932]])\n",
      "tensor([[-0.4797,  0.0366, -1.0991],\n",
      "        [-0.5294, -1.1353,  1.1166],\n",
      "        [-0.3947, -0.8635,  0.2264],\n",
      "        [-0.2832, -0.0005, -0.2650],\n",
      "        [-0.7224, -0.0999, -0.8005]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x+y)\n",
    "# print(torch.add(x, y)) # 완전히 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2973,  0.4552, -0.3719],\n",
      "        [ 0.3067, -0.4941,  2.4571],\n",
      "        [ 0.5993, -0.6067,  1.4955],\n",
      "        [ 0.1476,  0.1020, -0.8184],\n",
      "        [-0.3789,  0.5761, -1.0932]])\n"
     ]
    }
   ],
   "source": [
    "# 더하기 : 파라미터로(결과가 저장되는) 결과 텐서(output tensor) 이용\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out = result) # result값에 저장\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 텐서를 제자리에서 변조하는 연산은 _문자를 이용해 postfix(연산자를 피연산자의 뒷쪽에 표시)로 표기한다.\n",
    "* 예를 들면 x.copy_(y)와 x.t_()는 x를 변경시킨다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2973,  0.4552, -0.3719],\n",
      "        [ 0.3067, -0.4941,  2.4571],\n",
      "        [ 0.5993, -0.6067,  1.4955],\n",
      "        [ 0.1476,  0.1020, -0.8184],\n",
      "        [-0.3789,  0.5761, -1.0932]])\n"
     ]
    }
   ],
   "source": [
    "# 더하기 : 제자리(in-place)\n",
    "# adds x to y\n",
    "y.add_(x); print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Size------\n",
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n",
      "----------Tensor Y (1 x 16)----------\n",
      "tensor([ 0.3317, -0.6793,  0.9911,  0.6814,  0.2295, -0.6745, -1.1170, -1.3424,\n",
      "        -0.1847, -1.4813,  1.1554,  1.5671,  0.1196,  1.1485, -0.0880,  0.0414])\n",
      "----------Tensor Z (2 x 8)----------\n",
      "tensor([[ 0.3317, -0.6793,  0.9911,  0.6814,  0.2295, -0.6745, -1.1170, -1.3424],\n",
      "        [-0.1847, -1.4813,  1.1554,  1.5671,  0.1196,  1.1485, -0.0880,  0.0414]])\n"
     ]
    }
   ],
   "source": [
    "# Resizing : 텐서 사이즈를 재변경하거나, 모양(shape)을 변경하고 싶다면 view 사용\n",
    "x = torch.randn(4, 4) # 4*4 randnormalize tensor\n",
    "y = x.view(16) # reshape to 1*16 tensor\n",
    "z = x.view(-1, 8) # 2 * 8 Tensor (-1 은 어떠한 인자도 받을수 있기 때문에 자동적으로 2)\n",
    "print('------Size------')\n",
    "print(x.size(), y.size(), z.size())\n",
    "print('----------Tensor Y (1 x 16)----------')\n",
    "print(y)\n",
    "print('----------Tensor Z (2 x 8)----------')\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy 변환 (Bridge)\n",
    "* 토치 텐서와 배열은 근본적으로 메모리 위치를 공유하기 때문에 하나를 변경하면 다른 하나도 변경된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Tensor -> Array\n",
    "a = torch.ones(5) # 1로 가득한 1*5 Tensor\n",
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1) # a도 변한다, array로 했기 때문에 b도 변한다. a를 참조하고 있는 b도 변한다\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Array -> Tensor\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GPU Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서는 .to 메소드를 이용해 (CUDA를 지원하는)그 어떠한 디바이스로도 옮길 수 있다\n",
    "# let us run this cell only if CUDA is available\n",
    "# We will use 'torch.device' objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    y = torch.ones_like(x, device=device)   # a CUDA device object\n",
    "    x = x.to(device)                        # directly create a tensor on GPU\n",
    "    z = x + y                               # or just use strings ''.to('cuda')\n",
    "    print(z)\n",
    "    print(z.to('cpu', torch.double))        # ''.to'' cna also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd : 자동 미분 (Automatic Diff)\n",
    "**autograd** 패키지는 텐서의 모든 연사에 대하여 자동 미분을 제공한다. 이 패키지는 실행 시점에 정의되는(define-by-run) 프레임워크인데 다시 말하면 코드가 어떻게 실행되는지에 따라 역전파(backprop)가 정의되며, 각각의 반복마다 역전파가 달라질 수 있다는 것이다.\n",
    "\n",
    "* 텐서플로는 정의된 다음 실행되는(defined-and-run) 프레임워크인데 이는 그래프 구조에서 미리 조건과 반복을 정의하고 나서 실행된다.\n",
    "* PyTorch를 비롯한 Chainer, DyNet 등은 define-by-run 프레임워크이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.Tensor는 패키지에서 가장 중심이 되는 클래스\n",
    "\n",
    "-- 텐서의 속성 중 하나인 **.requires_grad**를 **True**로 세팅하면, 텐서의 모든 연산에 대해 추적을 시작한다.\n",
    "\n",
    "-- 계산 작업이 모두 수행됐다면 **.backward()** 를 호출하여 모든 그라디언트들을 자동으로 계산할 수 있다.\n",
    "\n",
    "-- 이 텐서를 위한 그라디언트는 **.grad** 속성에 누적되어 저장된다.\n",
    "\n",
    "-- 텐서에 대해 기록(history) 추적을 중지하려면 **.detach()** 를 호출해 현재의 계산 기록으로부터 분리시키고 이후에 일어나는 계산들은 추적되지 않게 할 수 있다.\n",
    "\n",
    "-- 기록 추적(및 메모리 사용)에 대해 방지를 하려면, 코드 블럭을 **with torch.no_grad():** 로 래핑(wrap)할 수 있다. 이는 특히 모델을 평가할 때 엄청난 도움이 되는데, 왜냐하면 모델은 requires_grad=True 속성이 적용된 학습 가능한 파라미터를 가지고 있을 수 있으나 우리는 그라디언트가 필요하지 않기 때문이다.\n",
    "\n",
    "-- 자동 미분을 위해 매우 중요한 클래스가 하나 더 있는데 바로 **Function**이다.\n",
    "\n",
    "-- **Tensor**와 **Function**은 상호 연결되어 있으며, 비순환(비주기) 그래프를 생성하는데, 이 그래프는 계싼 기록 전체에 대하여 인코딩을 수행한다. 각 변수는 **Tensor**를 생성한 **Function**을 참조하는 **.grad_fn** 속성을 가지고 있다. (단, 사용자에 의해 생성된 텐서는 제외한다 ; 해당 텐서들은 **grad_fn** 자체가 **None** 상태이다)\n",
    "\n",
    "-- 만약 도함수(derivatives)들을 계산하고 싶다면, Tensor의 **.backward()** 를 호출하면 된다. 만약 **Tensor**가 스칼라 형태라면, **backward()** 사용에 있어 그 어떠한 파라미터도 필요하지 않는다. 그러나 한 개 이상의 요소를 가지고 있다면 올바른 모양(matching shape)의 텐서인 **gradient** 파라미터를 명시할 필요가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# 텐서를 생성하고 requires~ 로 세팅해 계산 추적한다.\n",
    "# requires_grad = gradient update가 가능한 변수\n",
    "x = torch.ones(2, 2, requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward>)\n",
      "<AddBackward object at 0x10be8e550>\n"
     ]
    }
   ],
   "source": [
    "y = x+2\n",
    "print(y)\n",
    "# y는 연산의 결과로서 생성된 것이므로 grad_fn을 가지고 있다.\n",
    "print(y.grad_fn) # gradient을 Update할 때 편미분이 필요, 편미분을 하기 위해서 function을 참조해야한다.\n",
    "# AddBackward : + 연산을 통해 gradient update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9., 9.],\n",
      "        [9., 9.]], grad_fn=<ThMulBackward>) tensor(36., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y*y\n",
    "out = z.sum()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **requires_grad(...)** 은 이미 존재하는 텐서의 **requires_grad** 플래그를 제자리(in-place)에서 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----1st-----\n",
      "False\n",
      "-----2nd-----\n",
      "True\n",
      "-----3rd-----\n",
      "<SumBackward0 object at 0x000001CD86073160>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print('-----1st-----')\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print('-----2nd-----')\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print('-----3rd-----')\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그라디언트 (Gradients)\n",
    "* 다시 돌아와서 이제 **out**은 하나의 스칼라 값을 가지고 있기 때문에 **out.bacward()**는 **out.backward(torch.tensor(1))**와 동등한 결과를 리턴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 6.],\n",
      "        [6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad) # gradient값 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3.], grad_fn=<MulBackward>)\n",
      "tensor([ 0.6000,  6.0000, 60.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, requires_grad=True)\n",
    "y = x**2\n",
    "z = y*3\n",
    "print(z)\n",
    "gradient = torch.tensor([0.1, 1, 10], dtype=torch.float) # Tensor, 0.1, 1, 10 형태에 맞춰서 backpropagation\n",
    "# backward 인자가 곱해져 그라디언트 값 출력\n",
    "z.backward(gradient)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn과 nn.functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn의 기능들 (weight를 자동 설정해줌, conv2d의 경우)\n",
    "* Parameters    \n",
    "* Linear    \n",
    "* Container    \n",
    "* Dropout    \n",
    "* Conv  \n",
    "* Sparse    \n",
    "* Pooling    \n",
    "* Distance    \n",
    "* Padding    \n",
    "* Loss  \n",
    "* Non-linear Activation    \n",
    "* Vision    \n",
    "* Normalization  \n",
    "* Data Parallel   \n",
    "* Utilities   \n",
    "* Recurrent  \n",
    "\n",
    "### nn.functional의 기능들(외부에서 만든 filter, weight를 넣어야 함)\n",
    "* Conv    \n",
    "* Pooling   \n",
    "* Non-linear activation   \n",
    "* Normalization  \n",
    "* Linear function(=fully connected layer)  \n",
    "* Dropout  \n",
    "* Distance\n",
    "* Loss   \n",
    "* Vision  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#### nn.functional ####\n",
    "inp = torch.ones(1,1,3,3, requires_grad = True)\n",
    "filter = torch.ones(1,1,3,3)\n",
    "print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----output----\n",
      "tensor([[[[9.]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "----grad_fn----\n",
      "<ThnnConv2DBackward object at 0x1074398d0>\n",
      "----Gradient----\n",
      "tensor([[[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "out = F.conv2d(inp, filter)\n",
    "print(\"----output----\")\n",
    "print(out)\n",
    "out.backward()\n",
    "print(\"----grad_fn----\")\n",
    "print(out.grad_fn)\n",
    "print(\"----Gradient----\")\n",
    "print(inp.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----output----\n",
      "tensor([[[[18.]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "----grad_fn----\n",
      "<ThnnConv2DBackward object at 0x10be8eda0>\n",
      "----Gradient----\n",
      "tensor([[[[3., 3., 3.],\n",
      "          [3., 3., 3.],\n",
      "          [3., 3., 3.]]]])\n"
     ]
    }
   ],
   "source": [
    "filter += 1\n",
    "out = F.conv2d(inp, filter)\n",
    "print(\"----output----\")\n",
    "print(out)\n",
    "out.backward()\n",
    "print(\"----grad_fn----\")\n",
    "print(out.grad_fn)\n",
    "print(\"----Gradient----\")\n",
    "print(inp.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.1853, -0.1243,  0.2386],\n",
       "          [-0.0038,  0.1217,  0.1274],\n",
       "          [-0.2844, -0.2196,  0.0278]]]], requires_grad=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### nn ####\n",
    "inp = torch.ones(1,1,3,3, requires_grad = True)\n",
    "func = nn.Conv2d(1, 1, 3) # (input_channel, output_channel, filter_size or kernel_size)\n",
    "func.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.4743]]]], grad_fn=<ThnnConv2DBackward>)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "out = func(inp)\n",
    "print(out)\n",
    "# nn.Conv2d는 자동으로 bias를 포함시키기 때문에 값이 위의 weight 총합 값과 다름\n",
    "print(inp.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.1853, -0.1243,  0.2386],\n",
      "          [-0.0038,  0.1217,  0.1274],\n",
      "          [-0.2844, -0.2196,  0.0278]]]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(inp.grad) # 그냥 weight 자체가 산출됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- ReLU activation ----\n",
      "tensor([[[[0.4413, 0.1265, 0.1518],\n",
      "          [0.4792, 0.3916, 0.3039],\n",
      "          [0.0000, 0.0000, 2.1641]]]])\n",
      "---- Max Pooling 2 x 2 by stride 1 ----\n",
      "tensor([[[[0.4792, 0.3916],\n",
      "          [0.4792, 2.1641]]]])\n",
      "---- Sigmoid ----\n",
      "tensor([[[[0.6176, 0.5967],\n",
      "          [0.6176, 0.8970]]]])\n",
      "---- Tanh of Max Pooled image ----\n",
      "tensor([[[[0.4456, 0.3727],\n",
      "          [0.4456, 0.9740]]]])\n",
      "---- Average Pooling 2 x 2 by stride 1 ----\n",
      "tensor([[[[0.5595]]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justin/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/Users/justin/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "### 다양한 활성함수들\n",
    "inp2 = torch.randn(1,1,3,3)\n",
    "a = F.relu(inp2) # F = torch.nn.functional\n",
    "print(\"---- ReLU activation ----\")\n",
    "print(a)\n",
    "b = nn.MaxPool2d(2, stride=1)\n",
    "print(\"---- Max Pooling 2 x 2 by stride 1 ----\")\n",
    "bb = b(a)\n",
    "print(bb)\n",
    "c = F.sigmoid(bb)\n",
    "print(\"---- Sigmoid ----\")\n",
    "print(c)\n",
    "d = F.tanh(bb)\n",
    "print(\"---- Tanh of Max Pooled image ----\")\n",
    "print(d)\n",
    "e = nn.AvgPool2d(2, stride=1)\n",
    "print(\"---- Average Pooling 2 x 2 by stride 1 ----\")\n",
    "ee = e(d)\n",
    "print(ee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 일련의 과정을 한 번에 클래스로 묶어서 \n",
    "# 처리하도록 직접 모델을 설계한다!\n",
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        self.Max_pool = nn.MaxPool2d(2, stride=1)\n",
    "        self.Avg_pool = nn.AvgPool2d(2, stride=1)\n",
    "    def forward(self, x): # 한줄한줄 Layer 의미 \n",
    "        x = F.relu(x) # 1~2 : Relu\n",
    "        x = self.Max_pool(x) # 2~3 : Max Pooling\n",
    "        x = F.tanh(x) # 3~4 : tanh\n",
    "        x = self.Avg_pool(x) # 4~5 : Average Pooling\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justin/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5595]]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = model()\n",
    "net(inp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model(\n",
       "  (Max_pool): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Avg_pool): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justin/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.5800]]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp2 = torch.randn(1,1,3,3)\n",
    "net = model()\n",
    "net(inp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model(\n",
       "  (Max_pool): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (Avg_pool): AvgPool2d(kernel_size=2, stride=1, padding=0)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
